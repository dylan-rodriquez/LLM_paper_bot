{
  "papers": [
    {
      "title": "Query Routing for Retrieval-Augmented Language Models",
      "authors": [
        "Jiarui Zhang, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Guihai Chen"
      ],
      "abstract": "arXiv:2505.23052v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) significantly improves the performance of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying response quality across LLMs under RAG necessitates intelligent routing mechanisms, which select the most suitable model for each query from multiple retrieval-augmented LLMs via a dedicated router model. We observe that external documents dynamically affect LLMs' ability to answer queries, while existing routing methods, which rely on static parametric knowledge representations, exhibit suboptimal performance in RAG scenarios. To address this, we formally define the new retrieval-augmented LLM routing problem, incorporating the influence of retrieved documents into the routing framework. We propose RAGRouter, a RAG-aware routing design, which leverages document embeddings and RAG capability embeddings with contrastive learning to capture knowledge representation shifts and enable informed routing decisions. Extensive experiments on diverse knowledge-intensive tasks and retrieval settings show that RAGRouter outperforms the best individual LLM by 3.61% on average and existing routing methods by 3.29%-9.33%. With an extended score-threshold-based mechanism, it also achieves strong performance-efficiency trade-offs under low-latency constraints.",
      "url": "https://arxiv.org/abs/2505.23052",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 37.75,
      "innovation_score": 30,
      "impact_score": 16,
      "sentiment_score": 55.0,
      "keywords": [
        "routing",
        "rag",
        "retrieval",
        "augmented",
        "knowledge",
        "llms",
        "retrieval augmented",
        "performance",
        "documents",
        "embeddings"
      ],
      "subject_classification": "knowledge",
      "justification": "High innovation indicators (score: 30); Contains key LLM terms (bonus: 10)",
      "paper_id": "12075bcfe291ba6a0dc7d470f18820d7"
    },
    {
      "title": "ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation",
      "authors": [
        "Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong"
      ],
      "abstract": "arXiv:2502.15543v2 Announce Type: replace \nAbstract: Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All code will be released via GitHub.",
      "url": "https://arxiv.org/abs/2502.15543",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 37.13,
      "innovation_score": 20,
      "impact_score": 24,
      "sentiment_score": 56.9,
      "keywords": [
        "knowledge",
        "generation",
        "faithfulness",
        "internal",
        "external",
        "ffns",
        "parametric",
        "parammute",
        "augmented",
        "augmented generation"
      ],
      "subject_classification": "knowledge",
      "justification": "Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)",
      "paper_id": "cffea00d2a82804de78275092b181c5f"
    },
    {
      "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora",
      "authors": [
        "Jiaxin Bai, Wei Fan, Qi Hu, Qing Zong, Chunyang Li, Hong Ting Tsang, Hongyu Luo, Yauwai Yim, Haoyu Huang, Xiao Zhou, Feng Qin, Tianshi Zheng, Xi Peng, Xin Yao, Huiwen Yang, Leijie Wu, Yi Ji, Gong Zhang, Renhai Chen, Yangqiu Song"
      ],
      "abstract": "arXiv:2505.23628v1 Announce Type: new \nAbstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 95\\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.",
      "url": "https://arxiv.org/abs/2505.23628",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 36.57,
      "innovation_score": 30,
      "impact_score": 8,
      "sentiment_score": 52.85,
      "keywords": [
        "knowledge",
        "schemas",
        "induction",
        "schema",
        "schema induction",
        "autonomous",
        "autonomous knowledge",
        "autoschemakg",
        "billion",
        "construction"
      ],
      "subject_classification": "knowledge",
      "justification": "High innovation indicators (score: 30); Contains key LLM terms (bonus: 10)",
      "paper_id": "24ab540388b6d3a902007a535e6de39c"
    },
    {
      "title": "ConfQA: Answer Only If You Are Confident",
      "authors": [
        "Yin Huang, Yifan Ethan Xu, Kai Sun, Vera Yan, Alicia Sun, Haidar Khan, Jimmy Nguyen, Mohammad Kachuee, Zhaojiang Lin, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong"
      ],
      "abstract": "arXiv:2506.07309v1 Announce Type: new \nAbstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit \"I am unsure\". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt \"answer only if you are confident\" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.",
      "url": "https://arxiv.org/abs/2506.07309",
      "published_date": "2025-06-10T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 90,
      "innovation_score": 75,
      "impact_score": 80,
      "sentiment_score": 92,
      "keywords": [
        "knowledge",
        "answer",
        "confqa",
        "answer confident",
        "confidence",
        "confident",
        "factual",
        "factual statements",
        "framework",
        "hallucination"
      ],
      "subject_classification": "knowledge",
      "justification": "This paper addresses the critical problem of hallucination in LLMs, a major barrier to their reliable deployment. The ConfQA strategy, particularly the 'answer only if confident' prompt and leveraging knowledge graph attributes, appears to be a significant improvement over existing methods, achieving a substantial reduction in hallucination rates. The mention of a 'Dual Neural Knowledge framework' suggests further development and potential for broader applicability, indicating a strong likelihood of positive reception within the community.",
      "paper_id": "1c144474e835eac1c85efb4a6a19a89c"
    }
  ],
  "last_updated": "2025-06-10T09:29:56.346123"
}