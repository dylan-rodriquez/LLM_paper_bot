# ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation

**Authors:** Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 37.1/100

## Abstract

arXiv:2502.15543v2 Announce Type: replace 
Abstract: Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All code will be released via GitHub.

## Analysis

**Innovation Score:** 20.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 56.9/100

**Justification:** Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)

## Keywords

knowledge, generation, faithfulness, internal, external, ffns, parametric, parammute, augmented, augmented generation

## Links

- [Paper URL](https://arxiv.org/abs/2502.15543)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
