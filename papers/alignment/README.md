# Alignment Papers

This directory contains papers related to alignment in large language models and AI.

## Papers (7 total)

### SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues

**Score:** 92.0 | **Published:** 2025-06-03 | **Authors:** Martin Kuo, Jianyi Zhang, Aolin Ding, Louis DiValentin, Amin Hass, Benjamin F Morris, Isaac Jacobson, Randolph Linderman, James Kiessling, Nicolas Ramos, Bhavna Gopal, Maziyar Baran Pouyan, Changwei Liu, Hai Li, Yiran Chen

arXiv:2506.00668v1 Announce Type: new 
Abstract: Malicious attackers can exploit large language models (LLMs) by engaging them in multi-turn dialogues to achieve harmful objectives, posing significant...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2506.00668) | [ğŸ“ Analysis](433b5f82ca0e31c4eaed3e5912abd086.md)

---

### Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race

**Score:** 90.0 | **Published:** 2025-06-03 | **Authors:** Lihao Sun, Chengzhi Mao, Valentin Hofmann, Xuechunzi Bai

arXiv:2506.00253v1 Announce Type: new 
Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association ...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2506.00253) | [ğŸ“ Analysis](bbccf892506362693b4a6c00473066fb.md)

---

### SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents

**Score:** 47.6 | **Published:** 2025-05-30 | **Authors:** Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You

arXiv:2505.23559v1 Announce Type: new 
Abstract: Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critic...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23559) | [ğŸ“ Analysis](2e12e9f10c33e2a9dbcf638c4a54478f.md)

---

### Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models

**Score:** 44.7 | **Published:** 2025-05-30 | **Authors:** Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin

arXiv:2505.23404v1 Announce Type: new 
Abstract: Adversarial attacks on Large Language Models (LLMs) via jailbreaking techniques-methods that circumvent their built-in safety and ethical constraints-h...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23404) | [ğŸ“ Analysis](9eae0e3840c724306b1ddc667f7e7745.md)

---

### DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing

**Score:** 42.2 | **Published:** 2025-05-30 | **Authors:** Yi Wang, Fenghua Weng, Sibei Yang, Zhan Qin, Minlie Huang, Wenjie Wang

arXiv:2502.11647v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversaria...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2502.11647) | [ğŸ“ Analysis](af885fd893f2d38a2a90b5039080b1d9.md)

---

### Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment

**Score:** 37.9 | **Published:** 2025-05-30 | **Authors:** Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang

arXiv:2501.09620v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learn...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2501.09620) | [ğŸ“ Analysis](68da58ccb4c0dd5177e46296777c2f85.md)

---

### Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data

**Score:** 35.4 | **Published:** 2025-05-30 | **Authors:** Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang

arXiv:2505.23114v1 Announce Type: new 
Abstract: Human preference data plays a critical role in aligning large language models (LLMs) with human values. However, collecting such data is often expensiv...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23114) | [ğŸ“ Analysis](27dad59e7ac7e53184138273941c05bf.md)

---


*Last updated: 2025-06-03 09:29:41 UTC*
