# Alignment Papers

This directory contains papers related to alignment in large language models and AI.

## Papers (7 total)

### A Computational Approach to Improving Fairness in K-means Clustering

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Guancheng Zhou, Haiping Xu, Hongkang Xu, Chenyu Li, Donghui Yan

arXiv:2505.22984v1 Announce Type: new 
Abstract: The popular K-means clustering algorithm potentially suffers from a major weakness for further analysis or interpretation. Some cluster may have dispro...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.22984) | [ğŸ“ Analysis](189e48f9ea843b5f9b04cbf4f141da71.md)

---

### Towards Reward Fairness in RLHF: From a Resource Allocation Perspective

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Sheng Ouyang, Yulan Hu, Ge Chen, Qingyang Li, Fuzheng Zhang, Yong Liu

arXiv:2505.23349v1 Announce Type: new 
Abstract: Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards ...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23349) | [ğŸ“ Analysis](452804619589597a6788bc5049461836.md)

---

### SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents

**Score:** 47.6 | **Published:** 2025-05-30 | **Authors:** Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You

arXiv:2505.23559v1 Announce Type: new 
Abstract: Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critic...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23559) | [ğŸ“ Analysis](2e12e9f10c33e2a9dbcf638c4a54478f.md)

---

### Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models

**Score:** 44.7 | **Published:** 2025-05-30 | **Authors:** Mingyu Yu, Wei Wang, Yanjie Wei, Sujuan Qin

arXiv:2505.23404v1 Announce Type: new 
Abstract: Adversarial attacks on Large Language Models (LLMs) via jailbreaking techniques-methods that circumvent their built-in safety and ethical constraints-h...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23404) | [ğŸ“ Analysis](9eae0e3840c724306b1ddc667f7e7745.md)

---

### DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing

**Score:** 42.2 | **Published:** 2025-05-30 | **Authors:** Yi Wang, Fenghua Weng, Sibei Yang, Zhan Qin, Minlie Huang, Wenjie Wang

arXiv:2502.11647v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversaria...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2502.11647) | [ğŸ“ Analysis](af885fd893f2d38a2a90b5039080b1d9.md)

---

### Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment

**Score:** 37.9 | **Published:** 2025-05-30 | **Authors:** Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang

arXiv:2501.09620v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learn...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2501.09620) | [ğŸ“ Analysis](68da58ccb4c0dd5177e46296777c2f85.md)

---

### Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data

**Score:** 35.4 | **Published:** 2025-05-30 | **Authors:** Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang

arXiv:2505.23114v1 Announce Type: new 
Abstract: Human preference data plays a critical role in aligning large language models (LLMs) with human values. However, collecting such data is often expensiv...

[ğŸ“„ Full Paper](https://arxiv.org/abs/2505.23114) | [ğŸ“ Analysis](27dad59e7ac7e53184138273941c05bf.md)

---


*Last updated: 2025-05-31 09:25:12 UTC*
