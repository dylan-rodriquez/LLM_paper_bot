# Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models

**Authors:** Anirudh Bharadwaj, Chaitanya Malaviya, Nitish Joshi, Mark Yatskar

**Published:** 2025-06-06 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 90.0/100

## Abstract

arXiv:2506.05339v1 Announce Type: new 
Abstract: Language models serve as proxies for human preference judgements in alignment and evaluation, yet they exhibit systematic miscalibration, prioritizing superficial patterns over substantive qualities. This bias manifests as overreliance on features like length, structure, and style, leading to issues like reward hacking and unreliable evaluations. Evidence suggests these biases originate in artifacts in human training data. In this work, we systematically investigate the relationship between training data biases and preference model miscalibration across five idiosyncratic features of language model generations: length, structure, jargon, sycophancy and vagueness. Using controlled counterfactual pairs, we first quantify the extent to which preference models favor responses with magnified biases (skew), finding this preference occurs in >60% of instances, and model preferences show high miscalibration (~40%) compared to human preferences. Notably, bias features only show mild negative correlations to human preference labels (mean r_human = -0.12) but show moderately strong positive correlations with labels from a strong reward model (mean r_model = +0.36), suggesting that models may overrely on spurious cues. To mitigate these issues, we propose a simple post-training method based on counterfactual data augmentation (CDA) using synthesized contrastive examples. Finetuning models with CDA reduces average miscalibration from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%, while maintaining overall RewardBench performance, showing that targeted debiasing is effective for building reliable preference models.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This paper tackles a crucial problem in LLM alignment â€“ the susceptibility of preference models to superficial features. The systematic investigation using controlled counterfactuals and quantification of bias skew and miscalibration are strong methodological points. The findings, with >60% skew and ~40% miscalibration, are significant and suggest a widespread issue needing attention. The work is well-defined and addresses a timely concern in the field.

## Keywords

preference, models, biases, human, miscalibration, model, data, features, preference models, training

## Links

- [Paper URL](https://arxiv.org/abs/2506.05339)

---
*Auto-generated on 2025-06-06 09:28:50 UTC*
