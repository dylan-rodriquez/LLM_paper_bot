# LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation

**Authors:** Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang

**Published:** 2025-06-05 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 92.0/100

## Abstract

arXiv:2506.04078v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.

## Analysis

**Innovation Score:** 78.0/100
**Impact Score:** 88.0/100  
**Sentiment Score:** 90.0/100

**Justification:** This paper addresses a critical need for robust evaluation of medical LLMs, moving beyond limitations of existing benchmarks. The use of real-world EHR data and physician validation significantly strengthens the benchmark's relevance and reliability. The LLM-as-Judge framework with checklist refinement is a promising approach to automated evaluation, and the human-machine agreement analysis adds further rigor. The high scores reflect the importance of the problem and the well-defined methodology.

## Keywords

medical, llmeval, llmeval med, llms, med, models, clinical, expert, real, based

## Links

- [Paper URL](https://arxiv.org/abs/2506.04078)

---
*Auto-generated on 2025-06-05 09:29:15 UTC*
