# Evaluating the Sensitivity of LLMs to Prior Context

**Authors:** Robert Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, Greig A. Cowan

**Published:** 2025-06-03 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 90.0/100

## Abstract

arXiv:2506.00069v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 95.0/100

**Justification:** This research addresses a crucial and timely problem in LLM deployment â€“ the degradation of performance in multi-turn contexts. The introduction of novel benchmarks to specifically measure this sensitivity is a strong methodological contribution. The reported performance drops, even for state-of-the-art models like GPT-4o, highlight a significant gap in current LLM capabilities and warrant further investigation, suggesting a positive reception from the community.

## Keywords

context, performance, llms, models, turn, benchmarks, multi, multi turn, sensitivity, accuracy

## Links

- [Paper URL](https://arxiv.org/abs/2506.00069)

---
*Auto-generated on 2025-06-03 09:29:41 UTC*
