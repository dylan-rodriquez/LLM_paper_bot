# EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge

**Authors:** Ruskin Raj Manku, Yuzhi Tang, Xingjian Shi, Mu Li, Alex Smola

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 80.0/100

## Abstract

arXiv:2505.23009v1 Announce Type: new 
Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation $\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the $\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 82.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This paper addresses a crucial gap in TTS evaluation â€“ the lack of benchmarks that adequately assess complex linguistic and prosodic challenges. The use of LLMs for both test case generation and automated evaluation (model-as-a-judge) is a strong methodological contribution. While the 'model-as-a-judge' approach isn't entirely novel, its application within a comprehensive, automatically generated benchmark is a significant step forward, and the focus on nuanced challenges is highly relevant.

## Keywords

emergenttts, tts, emergenttts eval, eval, model, complex, judge, model judge, prosodic, using

## Links

- [Paper URL](https://arxiv.org/abs/2505.23009)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
