# FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models

**Authors:** Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane

**Published:** 2025-06-04 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 90.0/100

## Abstract

arXiv:2506.02961v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 92.0/100

**Justification:** This paper addresses a highly relevant and timely problem – federated learning for LLMs – which is crucial for addressing data privacy and access limitations. The creation of a cross-domain benchmark (FlowerTune) is a significant contribution, providing a standardized way to evaluate LLM performance in FL settings. While FL itself isn't new, applying it specifically to LLMs and creating a comprehensive benchmark demonstrates strong potential for impact and positive community reception.

## Keywords

domain, llms, federated, tuning, fine, fine tuning, data, pre, pre trained, trained

## Links

- [Paper URL](https://arxiv.org/abs/2506.02961)

---
*Auto-generated on 2025-06-04 09:29:15 UTC*
