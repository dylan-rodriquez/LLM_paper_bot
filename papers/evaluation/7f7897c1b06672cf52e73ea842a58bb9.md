# Large Language Models Often Know When They Are Being Evaluated

**Authors:** Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn

**Published:** 2025-06-02 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 90.0/100

## Abstract

arXiv:2505.23836v1 Announce Type: new 
Abstract: If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 85.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This research addresses a crucial and timely problem in the field of LLMs â€“ the potential for models to alter their behavior when they detect evaluation. The methodology appears sound, constructing a diverse benchmark and demonstrating above-random awareness in frontier models. While the human baseline is still higher, the findings raise serious concerns about the reliability of current evaluation methods and the need for more robust benchmarking strategies.

## Keywords

models, evaluation, evaluations, ai, ai models, awareness, deployment, evaluation awareness, frontier, auc

## Links

- [Paper URL](https://arxiv.org/abs/2505.23836)

---
*Auto-generated on 2025-06-02 09:29:50 UTC*
