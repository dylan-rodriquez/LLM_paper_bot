# ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering

**Authors:** Jingxuan Wei, Nan Xu, Junnan Zhu, Yanni Hao, Gaowei Wu, Bihui Yu, Lei Wang

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 47.3/100

## Abstract

arXiv:2505.23242v1 Announce Type: new 
Abstract: Chart question answering (CQA) has become a critical multimodal task for evaluating the reasoning capabilities of vision-language models. While early approaches have shown promising performance by focusing on visual features or leveraging large-scale pre-training, most existing evaluations rely on rigid output formats and objective metrics, thus ignoring the complex, real-world demands of practical chart analysis. In this paper, we introduce ChartMind, a new benchmark designed for complex CQA tasks in real-world settings. ChartMind covers seven task categories, incorporates multilingual contexts, supports open-domain textual outputs, and accommodates diverse chart formats, bridging the gap between real-world applications and traditional academic benchmarks. Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM, that focuses on extracting key contextual elements, reducing noise, and enhancing the reasoning accuracy of multimodal large language models. Extensive evaluations on ChartMind and three representative public benchmarks with 14 mainstream multimodal models show our framework significantly outperforms the previous three common CQA paradigms: instruction-following, OCR-enhanced, and chain-of-thought, highlighting the importance of flexible chart understanding for real-world CQA. These findings suggest new directions for developing more robust chart reasoning in future research.

## Analysis

**Innovation Score:** 20.0/100
**Impact Score:** 64.0/100  
**Sentiment Score:** 51.5/100

**Justification:** Strong impact potential (score: 64); Contains key LLM terms (bonus: 10)

## Keywords

chart, real, real world, world, chartmind, cqa, multimodal, complex, models, new

## Links

- [Paper URL](https://arxiv.org/abs/2505.23242)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
