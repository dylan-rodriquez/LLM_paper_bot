# VERINA: Benchmarking Verifiable Code Generation

**Authors:** Zhe Ye, Zhengxu Yan, Jingxuan He, Timothe Kasriel, Kaiyu Yang, Dawn Song

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 90.0/100

## Abstract

arXiv:2505.23135v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often lack support for end-to-end verifiable code generation. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs, with one trial per task. We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 95.0/100

**Justification:** This paper addresses a crucial challenge in the rapidly evolving field of LLMs â€“ ensuring code correctness. The creation of a dedicated benchmark, Verina, specifically designed for verifiable code generation is a significant contribution. The use of Lean, a formal verification tool, and the inclusion of specifications and proofs demonstrate a rigorous approach, and the high number of curated tasks suggests substantial effort. The focus on a modular evaluation framework is also a strength.

## Keywords

code, generation, code generation, verifiable, verifiable code, verina, evaluation, llms, specifications, benchmark

## Links

- [Paper URL](https://arxiv.org/abs/2505.23135)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
