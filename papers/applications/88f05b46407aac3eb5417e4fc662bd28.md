# Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context

**Authors:** Nilanjana Das, Edward Raff, Aman Chadha, Manas Gaur

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 37.3/100

## Abstract

arXiv:2412.16359v3 Announce Type: replace 
Abstract: As the AI systems become deeply embedded in social media platforms, we've uncovered a concerning security vulnerability that goes beyond traditional adversarial attacks. It becomes important to assess the risks of LLMs before the general public use them on social media platforms to avoid any adverse impacts. Unlike obvious nonsensical text strings that safety systems can easily catch, our work reveals that human-readable situation-driven adversarial full-prompts that leverage situational context are effective but much harder to detect. We found that skilled attackers can exploit the vulnerabilities in open-source and proprietary LLMs to make a malicious user query safe for LLMs, resulting in generating a harmful response. This raises an important question about the vulnerabilities of LLMs. To measure the robustness against human-readable attacks, which now present a potent threat, our research makes three major contributions. First, we developed attacks that use movie scripts as situational contextual frameworks, creating natural-looking full-prompts that trick LLMs into generating harmful content. Second, we developed a method to transform gibberish adversarial text into readable, innocuous content that still exploits vulnerabilities when used within the full-prompts. Finally, we enhanced the AdvPrompter framework with p-nucleus sampling to generate diverse human-readable adversarial texts that significantly improve attack effectiveness against models like GPT-3.5-Turbo-0125 and Gemma-7b. Our findings show that these systems can be manipulated to operate beyond their intended ethical boundaries when presented with seemingly normal prompts that contain hidden adversarial elements. By identifying these vulnerabilities, we aim to drive the development of more robust safety mechanisms that can withstand sophisticated attacks in real-world applications.

## Analysis

**Innovation Score:** 10.0/100
**Impact Score:** 40.0/100  
**Sentiment Score:** 46.5/100

**Justification:** Strong impact potential (score: 40); Contains key LLM terms (bonus: 10)

## Keywords

adversarial, llms, prompts, readable, vulnerabilities, attacks, human, human readable, situational, systems

## Links

- [Paper URL](https://arxiv.org/abs/2412.16359)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
