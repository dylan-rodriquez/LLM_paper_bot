# Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks

**Authors:** Dongwoo Lee, Dong Bok Lee, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim, Hae Beom Lee

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.23032v1 Announce Type: new 
Abstract: Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.

## Analysis

**Innovation Score:** 72.0/100
**Impact Score:** 75.0/100  
**Sentiment Score:** 80.0/100

**Justification:** This paper addresses a crucial limitation of current scaling law research â€“ the lack of uncertainty quantification. The Bayesian framework using Prior-Fitted Networks appears to be a novel approach to extrapolate scaling laws, offering a more robust method for resource allocation decisions. While the abstract is concise, it suggests a solid methodology and potential for significant impact, though further details are needed to fully assess rigor.

## Keywords

scaling, bayesian, extrapolation, laws, neural, neural scaling, scaling laws, law, prior, real

## Links

- [Paper URL](https://arxiv.org/abs/2505.23032)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
