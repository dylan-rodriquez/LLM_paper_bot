# Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs

**Authors:** Julia Belikova, Konstantin Polev, Rauf Parchiev, Dmitry Simakov

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 53.8/100

## Abstract

arXiv:2505.23299v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly deployed in industry applications, yet their reliability remains hampered by challenges in detecting hallucinations. While supervised state-of-the-art (SOTA) methods that leverage LLM hidden states -- such as activation tracing and representation analysis -- show promise, their dependence on extensively annotated datasets limits scalability in real-world applications. This paper addresses the critical bottleneck of data annotation by investigating the feasibility of reducing training data requirements for two SOTA hallucination detection frameworks: Lookback Lens, which analyzes attention head dynamics, and probing-based approaches, which decode internal model representations. We propose a methodology combining efficient classification algorithms with dimensionality reduction techniques to minimize sample size demands while maintaining competitive performance. Evaluations on standardized question-answering RAG benchmarks show that our approach achieves performance comparable to strong proprietary LLM-based baselines with only 250 training samples. These results highlight the potential of lightweight, data-efficient paradigms for industrial deployment, particularly in annotation-constrained scenarios.

## Analysis

**Innovation Score:** 20.0/100
**Impact Score:** 48.0/100  
**Sentiment Score:** 54.0/100

**Justification:** Strong impact potential (score: 48); Technical sophistication (score: 40); Contains key LLM terms (bonus: 15)

## Keywords

data, based, efficient, annotation, applications, data efficient, llm, llms, models, performance

## Links

- [Paper URL](https://arxiv.org/abs/2505.23299)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
