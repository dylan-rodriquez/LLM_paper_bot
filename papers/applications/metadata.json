{
  "papers": [
    {
      "title": "Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs",
      "authors": [
        "Julia Belikova, Konstantin Polev, Rauf Parchiev, Dmitry Simakov"
      ],
      "abstract": "arXiv:2505.23299v1 Announce Type: new \nAbstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly deployed in industry applications, yet their reliability remains hampered by challenges in detecting hallucinations. While supervised state-of-the-art (SOTA) methods that leverage LLM hidden states -- such as activation tracing and representation analysis -- show promise, their dependence on extensively annotated datasets limits scalability in real-world applications. This paper addresses the critical bottleneck of data annotation by investigating the feasibility of reducing training data requirements for two SOTA hallucination detection frameworks: Lookback Lens, which analyzes attention head dynamics, and probing-based approaches, which decode internal model representations. We propose a methodology combining efficient classification algorithms with dimensionality reduction techniques to minimize sample size demands while maintaining competitive performance. Evaluations on standardized question-answering RAG benchmarks show that our approach achieves performance comparable to strong proprietary LLM-based baselines with only 250 training samples. These results highlight the potential of lightweight, data-efficient paradigms for industrial deployment, particularly in annotation-constrained scenarios.",
      "url": "https://arxiv.org/abs/2505.23299",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 53.79,
      "innovation_score": 20,
      "impact_score": 48,
      "sentiment_score": 53.95,
      "keywords": [
        "data",
        "based",
        "efficient",
        "annotation",
        "applications",
        "data efficient",
        "llm",
        "llms",
        "models",
        "performance"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 48); Technical sophistication (score: 40); Contains key LLM terms (bonus: 15)",
      "paper_id": "c88e5b0cff0ff9ed803f286594783a99"
    },
    {
      "title": "SlimLLM: Accurate Structured Pruning for Large Language Models",
      "authors": [
        "Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang"
      ],
      "abstract": "arXiv:2505.22689v1 Announce Type: new \nAbstract: Large language models(LLMs) have garnered significant attention and demonstrated impressive capabilities in a wide range of applications. However, due to their enormous computational costs, the deployment and application of LLMs are often severely limited. To address this issue, structured pruning is an effective solution to compress the parameters of LLMs. Determining the importance of each sub-module in LLMs and minimizing performance loss are critical issues that need to be carefully addressed in structured pruning. In this paper, we propose an effective and fast structured pruning method named SlimLLM for large language models. For channel and attention head pruning, we evaluate the importance based on the entire channel or head, rather than merely aggregating the importance of individual elements within a sub-module. This approach enables a more holistic consideration of the interdependence among elements within the sub-module. In addition, we design a simple linear regression strategy for the output matrix to quickly recover performance. We also propose layer-based importance ratio to determine the pruning ratio for each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other methods and achieves state-of-the-art performance.",
      "url": "https://arxiv.org/abs/2505.22689",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 46.51,
      "innovation_score": 30,
      "impact_score": 32,
      "sentiment_score": 53.8,
      "keywords": [
        "pruning",
        "importance",
        "llms",
        "structured",
        "structured pruning",
        "based",
        "language",
        "language models",
        "large",
        "large language"
      ],
      "subject_classification": "applications",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 32); Contains key LLM terms (bonus: 15)",
      "paper_id": "b88bca68a1cf443b1e30b455da638114"
    },
    {
      "title": "Offline Learning for Combinatorial Multi-armed Bandits",
      "authors": [
        "Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee Joe-Wong, John C. S. Lui, Wei Chen"
      ],
      "abstract": "arXiv:2501.19300v2 Announce Type: replace \nAbstract: The combinatorial multi-armed bandit (CMAB) is a fundamental sequential decision-making framework, extensively studied over the past decade. However, existing work primarily focuses on the online setting, overlooking the substantial costs of online interactions and the readily available offline datasets. To overcome these limitations, we introduce Off-CMAB, the first offline learning framework for CMAB. Central to our framework is the combinatorial lower confidence bound (CLCB) algorithm, which combines pessimistic reward estimations with combinatorial solvers. To characterize the quality of offline datasets, we propose two novel data coverage conditions and prove that, under these conditions, CLCB achieves a near-optimal suboptimality gap, matching the theoretical lower bound up to a logarithmic factor. We validate Off-CMAB through practical applications, including learning to rank, large language model (LLM) caching, and social influence maximization, showing its ability to handle nonlinear reward functions, general feedback models, and out-of-distribution action samples that excludes optimal or even feasible actions. Extensive experiments on synthetic and real-world datasets further highlight the superior performance of CLCB.",
      "url": "https://arxiv.org/abs/2501.19300",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 43.88,
      "innovation_score": 20,
      "impact_score": 48,
      "sentiment_score": 54.4,
      "keywords": [
        "cmab",
        "combinatorial",
        "offline",
        "clcb",
        "datasets",
        "framework",
        "learning",
        "armed",
        "bound",
        "combinatorial multi"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 48); Contains key LLM terms (bonus: 10)",
      "paper_id": "c102fcd6d3acf1160a967ea43d2f0a06"
    },
    {
      "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions",
      "authors": [
        "Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman"
      ],
      "abstract": "arXiv:2503.22353v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent performance across multiple interaction rounds. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions. First, we propose a novel Position-Weighted Consistency (PWC) score that captures both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by incorporating model confidence signals into the generation process. Empirical results demonstrate that CARG significantly improves response stability without sacrificing accuracy, underscoring its potential for reliable LLM deployment in critical applications.",
      "url": "https://arxiv.org/abs/2503.22353",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 43.02,
      "innovation_score": 30,
      "impact_score": 40,
      "sentiment_score": 57.6,
      "keywords": [
        "consistency",
        "response",
        "llm",
        "stability",
        "carg",
        "confidence",
        "deployment",
        "domains",
        "evaluating",
        "framework"
      ],
      "subject_classification": "applications",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 40); Contains key LLM terms (bonus: 10)",
      "paper_id": "e5db119640754cb2ec64db833b93c795"
    },
    {
      "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs",
      "authors": [
        "Chiwan Park, Wonjun Jang, Daeryong Kim, Aelim Ahn, Kichang Yang, Woosung Hwang, Jihyeon Roh, Hyerin Park, Hyosun Wang, Min Seok Kim, Jihoon Kang"
      ],
      "abstract": "arXiv:2505.23006v1 Announce Type: new \nAbstract: The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents.",
      "url": "https://arxiv.org/abs/2505.23006",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 41.7,
      "innovation_score": 20,
      "impact_score": 40,
      "sentiment_score": 53.5,
      "keywords": [
        "conversational",
        "agents",
        "applications",
        "approach",
        "llms",
        "practical",
        "real",
        "real world",
        "research",
        "service"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 40); Contains key LLM terms (bonus: 10)",
      "paper_id": "cfee485585384ea65bc8e555cba8942c"
    },
    {
      "title": "Domain-Aware Tensor Network Structure Search",
      "authors": [
        "Giorgos Iacovides, Wuyang Zhou, Chao Li, Qibin Zhao, Danilo Mandic"
      ],
      "abstract": "arXiv:2505.23537v1 Announce Type: cross \nAbstract: Tensor networks (TNs) provide efficient representations of high-dimensional data, yet identification of the optimal TN structures, the so called tensor network structure search (TN-SS) problem, remains a challenge. Current state-of-the-art (SOTA) algorithms are computationally expensive as they require extensive function evaluations, which is prohibitive for real-world applications. In addition, existing methods ignore valuable domain information inherent in real-world tensor data and lack transparency in their identified TN structures. To this end, we propose a novel TN-SS framework, termed the tnLLM, which incorporates domain information about the data and harnesses the reasoning capabilities of large language models (LLMs) to directly predict suitable TN structures. The proposed framework involves a domain-aware prompting pipeline which instructs the LLM to infer suitable TN structures based on the real-world relationships between tensor modes. In this way, our approach is capable of not only iteratively optimizing the objective function, but also generating domain-aware explanations for the identified structures. Experimental results demonstrate that tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations compared to SOTA algorithms. Furthermore, we demonstrate that the LLM-enabled domain information can be used to find good initializations in the search space for sampling-based SOTA methods to accelerate their convergence while preserving theoretical performance guarantees.",
      "url": "https://arxiv.org/abs/2505.23537",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 39.99,
      "innovation_score": 20,
      "impact_score": 24,
      "sentiment_score": 52.45,
      "keywords": [
        "tn",
        "domain",
        "structures",
        "tensor",
        "function",
        "tn structures",
        "aware",
        "data",
        "domain aware",
        "domain information"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)",
      "paper_id": "3978d513cb8adbbae8e24e598975a290"
    },
    {
      "title": "Reducing Tool Hallucination via Reliability Alignment",
      "authors": [
        "Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, Kai Yu"
      ],
      "abstract": "arXiv:2412.04141v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have expanded their capabilities beyond language generation to interact with external tools, enabling automation and real-world applications. However, tool hallucinations, where models either select inappropriate tools or misuse them, pose significant challenges, leading to erroneous task execution, increased computational costs, and reduced system reliability. To systematically address this issue, we define and categorize tool hallucinations into two main types, tool selection hallucination and tool usage hallucination. To evaluate and mitigate these issues, we introduce RelyToolBench, which integrates specialized test cases and novel metrics to assess hallucination-aware task success and efficiency. Finally, we propose Relign, a reliability alignment framework that expands the tool-use action space to include indecisive actions, allowing LLMs to defer tool use, seek clarification, or adjust tool selection dynamically. Through extensive experiments, we demonstrate that Relign significantly reduces tool hallucinations, improves task reliability, and enhances the efficiency of LLM tool interactions.",
      "url": "https://arxiv.org/abs/2412.04141",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 38.69,
      "innovation_score": 20,
      "impact_score": 32,
      "sentiment_score": 54.7,
      "keywords": [
        "tool",
        "hallucination",
        "reliability",
        "hallucinations",
        "task",
        "tool hallucinations",
        "alignment",
        "efficiency",
        "language",
        "llms"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 32); Contains key LLM terms (bonus: 10)",
      "paper_id": "ba02c711f094188c5dd259056c6567c9"
    },
    {
      "title": "Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning",
      "authors": [
        "Yufeng Wang, Yiguang Bai, Tianqing Zhu, Ismail Ben Ayed, Jing Yuan"
      ],
      "abstract": "arXiv:2505.22684v1 Announce Type: cross \nAbstract: Community partitioning is crucial in network analysis, with modularity optimization being the prevailing technique. However, traditional modularity-based methods often overlook fairness, a critical aspect in real-world applications. To address this, we introduce protected group networks and propose a novel fairness-modularity metric. This metric extends traditional modularity by explicitly incorporating fairness, and we prove that minimizing it yields naturally fair partitions for protected groups while maintaining theoretical soundness. We develop a general optimization framework for fairness partitioning and design the efficient Fair Fast Newman (FairFN) algorithm, enhancing the Fast Newman (FN) method to optimize both modularity and fairness. Experiments show FairFN achieves significantly improved fairness and high-quality partitions compared to state-of-the-art methods, especially on unbalanced datasets.",
      "url": "https://arxiv.org/abs/2505.22684",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.SI"
      ],
      "significance_score": 37.68,
      "innovation_score": 30,
      "impact_score": 32,
      "sentiment_score": 59.65,
      "keywords": [
        "fairness",
        "modularity",
        "fair",
        "partitioning",
        "community",
        "community partitioning",
        "fairfn",
        "fast",
        "fast newman",
        "methods"
      ],
      "subject_classification": "applications",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 32); Technical sophistication (score: 35)",
      "paper_id": "b91f30ba16569be32399f61f8792d91e"
    },
    {
      "title": "Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context",
      "authors": [
        "Nilanjana Das, Edward Raff, Aman Chadha, Manas Gaur"
      ],
      "abstract": "arXiv:2412.16359v3 Announce Type: replace \nAbstract: As the AI systems become deeply embedded in social media platforms, we've uncovered a concerning security vulnerability that goes beyond traditional adversarial attacks. It becomes important to assess the risks of LLMs before the general public use them on social media platforms to avoid any adverse impacts. Unlike obvious nonsensical text strings that safety systems can easily catch, our work reveals that human-readable situation-driven adversarial full-prompts that leverage situational context are effective but much harder to detect. We found that skilled attackers can exploit the vulnerabilities in open-source and proprietary LLMs to make a malicious user query safe for LLMs, resulting in generating a harmful response. This raises an important question about the vulnerabilities of LLMs. To measure the robustness against human-readable attacks, which now present a potent threat, our research makes three major contributions. First, we developed attacks that use movie scripts as situational contextual frameworks, creating natural-looking full-prompts that trick LLMs into generating harmful content. Second, we developed a method to transform gibberish adversarial text into readable, innocuous content that still exploits vulnerabilities when used within the full-prompts. Finally, we enhanced the AdvPrompter framework with p-nucleus sampling to generate diverse human-readable adversarial texts that significantly improve attack effectiveness against models like GPT-3.5-Turbo-0125 and Gemma-7b. Our findings show that these systems can be manipulated to operate beyond their intended ethical boundaries when presented with seemingly normal prompts that contain hidden adversarial elements. By identifying these vulnerabilities, we aim to drive the development of more robust safety mechanisms that can withstand sophisticated attacks in real-world applications.",
      "url": "https://arxiv.org/abs/2412.16359",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 37.3,
      "innovation_score": 10,
      "impact_score": 40,
      "sentiment_score": 46.5,
      "keywords": [
        "adversarial",
        "llms",
        "prompts",
        "readable",
        "vulnerabilities",
        "attacks",
        "human",
        "human readable",
        "situational",
        "systems"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 40); Contains key LLM terms (bonus: 10)",
      "paper_id": "88f05b46407aac3eb5417e4fc662bd28"
    },
    {
      "title": "Improving Time Series Forecasting via Instance-aware Post-hoc Revision",
      "authors": [
        "Zhiding Liu, Mingyue Cheng, Guanhao Zhao, Jiqian Yang, Qi Liu, Enhong Chen"
      ],
      "abstract": "arXiv:2505.23583v1 Announce Type: new \nAbstract: Time series forecasting plays a vital role in various real-world applications and has attracted significant attention in recent decades. While recent methods have achieved remarkable accuracy by incorporating advanced inductive biases and training strategies, we observe that instance-level variations remain a significant challenge. These variations--stemming from distribution shifts, missing data, and long-tail patterns--often lead to suboptimal forecasts for specific instances, even when overall performance appears strong. To address this issue, we propose a model-agnostic framework, PIR, designed to enhance forecasting performance through Post-forecasting Identification and Revision. Specifically, PIR first identifies biased forecasting instances by estimating their accuracy. Based on this, the framework revises the forecasts using contextual information, including covariates and historical time series, from both local and global perspectives in a post-processing fashion. Extensive experiments on real-world datasets with mainstream forecasting models demonstrate that PIR effectively mitigates instance-level errors and significantly improves forecasting reliability.",
      "url": "https://arxiv.org/abs/2505.23583",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 36.94,
      "innovation_score": 30,
      "impact_score": 32,
      "sentiment_score": 55.95,
      "keywords": [
        "forecasting",
        "instance",
        "pir",
        "post",
        "series",
        "time",
        "time series",
        "accuracy",
        "forecasts",
        "framework"
      ],
      "subject_classification": "applications",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 32); Contains key LLM terms (bonus: 5)",
      "paper_id": "b666dbd2a9712e43666b415c3a10de5d"
    },
    {
      "title": "MedRAX: Medical Reasoning Agent for Chest X-ray",
      "authors": [
        "Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang"
      ],
      "abstract": "arXiv:2502.02673v2 Announce Type: replace-cross \nAbstract: Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX",
      "url": "https://arxiv.org/abs/2502.02673",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 36.65,
      "innovation_score": 20,
      "impact_score": 40,
      "sentiment_score": 53.25,
      "keywords": [
        "medrax",
        "models",
        "cxr",
        "medical",
        "agent",
        "art",
        "chest",
        "complex",
        "complex medical",
        "cxr interpretation"
      ],
      "subject_classification": "applications",
      "justification": "Strong impact potential (score: 40); Contains key LLM terms (bonus: 5)",
      "paper_id": "6dd963be5887714929773f696df6273d"
    },
    {
      "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems",
      "authors": [
        "Christopher Ormerod"
      ],
      "abstract": "arXiv:2505.22771v1 Announce Type: new \nAbstract: This study illustrates how incorporating feedback-oriented annotations into the scoring pipeline can enhance the accuracy of automated essay scoring (AES). This approach is demonstrated with the Persuasive Essays for Rating, Selecting, and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We integrate two types of feedback-driven annotations: those that identify spelling and grammatical errors, and those that highlight argumentative components. To illustrate how this method could be applied in real-world scenarios, we employ two LLMs to generate annotations -- a generative language model used for spell-correction and an encoder-based token classifier trained to identify and mark argumentative elements. By incorporating annotations into the scoring process, we demonstrate improvements in performance using encoder-based large language models fine-tuned as classifiers.",
      "url": "https://arxiv.org/abs/2505.22771",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 75,
      "innovation_score": 65,
      "impact_score": 70,
      "sentiment_score": 80,
      "keywords": [
        "annotations",
        "scoring",
        "argumentative",
        "automated",
        "feedback",
        "incorporating",
        "annotations scoring",
        "automated essay",
        "based",
        "elements"
      ],
      "subject_classification": "applications",
      "justification": "The paper addresses a relevant problem in educational technology \u2013 improving AES. Integrating feedback annotations is a logical and potentially effective approach. While the methodology appears sound (using LLMs for annotation and fine-tuning classifiers), the abstract doesn't detail the *magnitude* of improvement, which impacts the perceived novelty. The use of the PERSUADE corpus is a good choice, and the focus on argumentative components is valuable.",
      "paper_id": "1d97e0242cd43b7d43ec31379fa3ce89"
    },
    {
      "title": "Towards a More Generalized Approach in Open Relation Extraction",
      "authors": [
        "Qing Wang, Yuepei Li, Qiao Qiao, Kang Zhou, Qi Li"
      ],
      "abstract": "arXiv:2505.22801v1 Announce Type: new \nAbstract: Open Relation Extraction (OpenRE) seeks to identify and extract novel relational facts between named entities from unlabeled data without pre-defined relation schemas. Traditional OpenRE methods typically assume that the unlabeled data consists solely of novel relations or is pre-divided into known and novel instances. However, in real-world scenarios, novel relations are arbitrarily distributed. In this paper, we propose a generalized OpenRE setting that considers unlabeled data as a mixture of both known and novel instances. To address this, we propose MixORE, a two-phase framework that integrates relation classification and clustering to jointly learn known and novel relations. Experiments on three benchmark datasets demonstrate that MixORE consistently outperforms competitive baselines in known relation classification and novel relation clustering. Our findings contribute to the advancement of generalized OpenRE research and real-world applications.",
      "url": "https://arxiv.org/abs/2505.22801",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 75,
      "innovation_score": 70,
      "impact_score": 72,
      "sentiment_score": 80,
      "keywords": [
        "novel",
        "relation",
        "known",
        "openre",
        "data",
        "generalized",
        "known novel",
        "novel relations",
        "relations",
        "unlabeled"
      ],
      "subject_classification": "applications",
      "justification": "The paper addresses a realistic limitation of current OpenRE methods \u2013 the assumption of purely novel relations. Proposing a generalized setting and a two-phase framework (MixORE) to handle mixed data is a solid contribution. The reported performance improvements on benchmark datasets suggest a well-executed methodology, though the abstract lacks detail on the specifics of the clustering and classification techniques used. The problem is relevant and the approach appears sound, indicating a positive reception within the OpenRE community.",
      "paper_id": "b702bce377bd40a6577bf64dd1b7e482"
    },
    {
      "title": "Efficient Preimage Approximation for Neural Network Certification",
      "authors": [
        "Anton Bj\\\"orklund, Mykola Zaitsev, Marta Kwiatkowska"
      ],
      "abstract": "arXiv:2505.22798v1 Announce Type: new \nAbstract: The growing reliance on artificial intelligence in safety- and security-critical applications demands effective neural network certification. A challenging real-world use case is certification against ``patch attacks'', where adversarial patches or lighting conditions obscure parts of images, for example traffic signs. One approach to certification, which also gives quantitative coverage estimates, utilizes preimages of neural networks, i.e., the set of inputs that lead to a specified output. However, these preimage approximation methods, including the state-of-the-art PREMAP algorithm, struggle with scalability. This paper presents novel algorithmic improvements to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics. We demonstrate efficiency improvements of at least an order of magnitude on reinforcement learning control benchmarks, and show that our method scales to convolutional neural networks that were previously infeasible. Our results demonstrate the potential of preimage approximation methodology for reliability and robustness certification.",
      "url": "https://arxiv.org/abs/2505.22798",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 85,
      "innovation_score": 70,
      "impact_score": 80,
      "sentiment_score": 82,
      "keywords": [
        "certification",
        "neural",
        "approximation",
        "preimage",
        "preimage approximation",
        "demonstrate",
        "improvements",
        "network",
        "network certification",
        "networks"
      ],
      "subject_classification": "applications",
      "justification": "The paper addresses a crucial problem in AI safety \u2013 neural network certification against adversarial attacks, specifically patch attacks. Improving the scalability of preimage approximation methods like PREMAP is a significant step forward, and the reported order of magnitude efficiency gains are promising. While the improvements appear incremental (tighter bounds, adaptive sampling, improved heuristics), they are applied to a relevant and challenging problem, suggesting a positive reception within the community. The focus on reinforcement learning control benchmarks also adds to its practical relevance.",
      "paper_id": "ebc272c00daa9b83c860d6bedb2b8301"
    },
    {
      "title": "Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks",
      "authors": [
        "Dongwoo Lee, Dong Bok Lee, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim, Hae Beom Lee"
      ],
      "abstract": "arXiv:2505.23032v1 Announce Type: new \nAbstract: Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.",
      "url": "https://arxiv.org/abs/2505.23032",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 85,
      "innovation_score": 72,
      "impact_score": 75,
      "sentiment_score": 80,
      "keywords": [
        "scaling",
        "bayesian",
        "extrapolation",
        "laws",
        "neural",
        "neural scaling",
        "scaling laws",
        "law",
        "prior",
        "real"
      ],
      "subject_classification": "applications",
      "justification": "This paper addresses a crucial limitation of current scaling law research \u2013 the lack of uncertainty quantification. The Bayesian framework using Prior-Fitted Networks appears to be a novel approach to extrapolate scaling laws, offering a more robust method for resource allocation decisions. While the abstract is concise, it suggests a solid methodology and potential for significant impact, though further details are needed to fully assess rigor.",
      "paper_id": "079d9a9eacea5731c70123bbc17180e5"
    },
    {
      "title": "An Empirical Study of Federated Prompt Learning for Vision Language Model",
      "authors": [
        "Zhihao Wang, Wenke Huang, Tian Chen, Zekun Shi, Guancheng Wan, Yu Qiao, Bin Yang, Jian Wang, Bing Li, Mang Ye"
      ],
      "abstract": "arXiv:2505.23024v1 Announce Type: new \nAbstract: The Vision Language Model (VLM) excels in aligning vision and language representations, and prompt learning has emerged as a key technique for adapting such models to downstream tasks. However, the application of prompt learning with VLM in federated learning (\\fl{}) scenarios remains underexplored. This paper systematically investigates the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under data heterogeneity challenges, including label skew and domain shift. We conduct extensive experiments to evaluate the impact of various \\fl{} and prompt configurations, such as client scale, aggregation strategies, and prompt length, to assess the robustness of Federated Prompt Learning (FPL). Furthermore, we explore strategies for enhancing prompt learning in complex scenarios where label skew and domain shift coexist, including leveraging both prompt types when computational resources allow. Our findings offer practical insights into optimizing prompt learning in federated settings, contributing to the broader deployment of VLMs in privacy-preserving environments.",
      "url": "https://arxiv.org/abs/2505.23024",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 82,
      "innovation_score": 65,
      "impact_score": 75,
      "sentiment_score": 80,
      "keywords": [
        "prompt",
        "learning",
        "prompt learning",
        "federated",
        "language",
        "vision",
        "vision language",
        "domain",
        "domain shift",
        "federated prompt"
      ],
      "subject_classification": "applications",
      "justification": "This paper addresses a relevant and growing area \u2013 federated learning with Vision Language Models. The systematic investigation of prompt learning differences (LPT vs. VPT) under data heterogeneity is a strong point. While the problem is significant, the abstract doesn't suggest a radically new approach, leaning more towards empirical analysis and configuration exploration. The exploration of combining prompt types is a positive step.",
      "paper_id": "07fba6ed2b33dd4214eef59479921296"
    },
    {
      "title": "CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables",
      "authors": [
        "Pengfei Zhou, Yunlong Liu, Junli Liang, Qi Song, Xiangyang Li"
      ],
      "abstract": "arXiv:2505.23116v1 Announce Type: new \nAbstract: Time series forecasting with exogenous variables is a critical emerging paradigm that presents unique challenges in modeling dependencies between variables. Traditional models often struggle to differentiate between endogenous and exogenous variables, leading to inefficiencies and overfitting. In this paper, we introduce CrossLinear, a novel Linear-based forecasting model that addresses these challenges by incorporating a plug-and-play cross-correlation embedding module. This lightweight module captures the dependencies between variables with minimal computational cost and seamlessly integrates into existing neural networks. Specifically, it captures time-invariant and direct variable dependencies while disregarding time-varying or indirect dependencies, thereby mitigating the risk of overfitting in dependency modeling and contributing to consistent performance improvements. Furthermore, CrossLinear employs patch-wise processing and a global linear head to effectively capture both short-term and long-term temporal dependencies, further improving its forecasting precision. Extensive experiments on 12 real-world datasets demonstrate that CrossLinear achieves superior performance in both short-term and long-term forecasting tasks. The ablation study underscores the effectiveness of the cross-correlation embedding module. Additionally, the generalizability of this module makes it a valuable plug-in for various forecasting tasks across different domains. Codes are available at https://github.com/mumiao2000/CrossLinear.",
      "url": "https://arxiv.org/abs/2505.23116",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 82,
      "innovation_score": 70,
      "impact_score": 75,
      "sentiment_score": 80,
      "keywords": [
        "forecasting",
        "crosslinear",
        "dependencies",
        "variables",
        "module",
        "term",
        "time",
        "correlation",
        "correlation embedding",
        "cross"
      ],
      "subject_classification": "applications",
      "justification": "The paper addresses a relevant problem in time series forecasting \u2013 handling exogenous variables \u2013 and proposes a novel approach (CrossLinear) that aims to improve performance and prevent overfitting. The 'plug-and-play' aspect and focus on direct dependencies are promising. While the abstract suggests a solid methodology, the full paper will need to demonstrate rigorous evaluation and comparison to existing methods to fully validate its claims. The focus on linear-based forecasting in a field increasingly dominated by neural networks is a slightly unusual choice, but the efficiency argument is compelling.",
      "paper_id": "83aa9c3b9ee5adeb4d652094853ce4c3"
    },
    {
      "title": "FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference",
      "authors": [
        "Aniruddha Nrusimha, William Brandon, Mayank Mishra, Yikang Shen, Rameswar Panda, Jonathan Ragan-Kelley, Yoon Kim"
      ],
      "abstract": "arXiv:2505.22758v1 Announce Type: new \nAbstract: The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for training and inference. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads contribute are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, a proof-of-concept kernel for accelerating single-batch inference for transformer-based large language models. Across various model sizes and quantizations settings, we observe nontrivial speedups compared to existing state-of-the-art inference kernels.",
      "url": "https://arxiv.org/abs/2505.22758",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 80,
      "innovation_score": 70,
      "impact_score": 75,
      "sentiment_score": 85,
      "keywords": [
        "inference",
        "batch",
        "kernels",
        "batch inference",
        "large",
        "applications",
        "compute",
        "existing",
        "flashformer",
        "kernel"
      ],
      "subject_classification": "applications",
      "justification": "The paper addresses a crucial bottleneck in LLM deployment \u2013 efficient low-batch inference. Focusing on memory bandwidth and kernel launch overheads is a smart move, given the increasing demand for edge deployment and latency-sensitive applications. While described as a 'proof-of-concept,' the observed speedups suggest a promising direction, and the clear problem statement and focus contribute to a strong initial presentation. The field is actively seeking inference optimizations, so positive reception is likely.",
      "paper_id": "567c7ca58b46169cb58205ff1f3ceef7"
    }
  ],
  "last_updated": "2025-05-31T09:25:12.841997"
}