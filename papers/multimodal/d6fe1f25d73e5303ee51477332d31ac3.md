# DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding

**Authors:** Yunhai Hu, Tianhua Xia, Zining Liu, Rahul Raman, Xingyu Liu, Bo Bao, Eric Sather, Vithursan Thangarasa, Sai Qian Zhang

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 35.7/100

## Abstract

arXiv:2505.19201v2 Announce Type: replace 
Abstract: Speculative decoding (SD) has emerged as a powerful method for accelerating autoregressive generation in large language models (LLMs), yet its integration into vision-language models (VLMs) remains underexplored. We introduce DREAM, a novel speculative decoding framework tailored for VLMs that combines three key innovations: (1) a cross-attention-based mechanism to inject intermediate features from the target model into the draft model for improved alignment, (2) adaptive intermediate feature selection based on attention entropy to guide efficient draft model training, and (3) visual token compression to reduce draft model latency. DREAM enables efficient, accurate, and parallel multimodal decoding with significant throughput improvement. Experiments across a diverse set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3, demonstrate up to 3.6x speedup over conventional decoding and significantly outperform prior SD baselines in both inference throughput and speculative draft acceptance length across a broad range of multimodal benchmarks. The code is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git

## Analysis

**Innovation Score:** 10.0/100
**Impact Score:** 8.0/100  
**Sentiment Score:** 59.6/100

**Justification:** Contains key LLM terms (bonus: 15)

## Keywords

decoding, draft, dream, model, speculative, attention, draft model, multimodal, speculative decoding, vlms

## Links

- [Paper URL](https://arxiv.org/abs/2505.19201)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
