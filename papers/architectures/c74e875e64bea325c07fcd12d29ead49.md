# Tensor Product Attention Is All You Need

**Authors:** Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew C Yao

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 38.4/100

## Abstract

arXiv:2501.06425v4 Announce Type: replace 
Abstract: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines, including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at the decoding stage enable processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.

## Analysis

**Innovation Score:** 30.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 54.3/100

**Justification:** High innovation indicators (score: 30); Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)

## Keywords

attention, tensor, tpa, efficiency, language, memory, multi, product, product attention, t6

## Links

- [Paper URL](https://arxiv.org/abs/2501.06425)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
