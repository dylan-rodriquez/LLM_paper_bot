# Architectures Papers

This directory contains papers related to architectures in large language models and AI.

## Papers (29 total)

### Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Ange-Clement Akazan, Verlon Roel Mbingui, Gnankan Landry Regis N'guessan, Issa Karambal

arXiv:2505.22686v1 Announce Type: new 
Abstract: Weather forecasting is crucial for managing risks and economic planning, particularly in tropical Africa, where extreme events severely impact liveliho...

[📄 Full Paper](https://arxiv.org/abs/2505.22686) | [📝 Analysis](8501a4163c8e084445cf2091e20cf4b1.md)

---

### Update Your Transformer to the Latest Release: Re-Basin of Task Vectors

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Filippo Rinaldi, Giacomo Capitani, Lorenzo Bonicelli, Donato Crisostomi, Federico Bolelli, Elisa Ficarra, Emanuele Rodol\`a, Simone Calderara, Angelo Porrello

arXiv:2505.22697v1 Announce Type: new 
Abstract: Foundation models serve as the backbone for numerous specialized models developed through fine-tuning. However, when the underlying pretrained model is...

[📄 Full Paper](https://arxiv.org/abs/2505.22697) | [📝 Analysis](e01a6b51c8755746c164edf73a99f04d.md)

---

### Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Ning Liu, Yue Yu

arXiv:2505.23106v1 Announce Type: new 
Abstract: Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largel...

[📄 Full Paper](https://arxiv.org/abs/2505.23106) | [📝 Analysis](da6ba7d02a5f095d1a6c0436d6afb9c4.md)

---

### Improving the Effective Receptive Field of Message-Passing Neural Networks

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Shahaf E. Finder, Ron Shapira Weber, Moshe Eliasof, Oren Freifeld, Eran Treister

arXiv:2505.23185v1 Announce Type: new 
Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for processing and analyzing graph-structured data. However, their effectiveness is o...

[📄 Full Paper](https://arxiv.org/abs/2505.23185) | [📝 Analysis](6d0fb86a86e775e3d1f6b95be17d4ae8.md)

---

### DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer

**Score:** 85.0 | **Published:** 2025-05-31 | **Authors:** Yekun Zhu, Min Tang, Zheng Ma

arXiv:2505.23190v1 Announce Type: new 
Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to address the steady-state Radiative Transfer Equation (RTE). The RTE is a ...

[📄 Full Paper](https://arxiv.org/abs/2505.23190) | [📝 Analysis](daab479378a9d2c51b9b82f9e02e622b.md)

---

### X-Factor: Quality Is a Dataset-Intrinsic Property

**Score:** 80.0 | **Published:** 2025-05-31 | **Authors:** Josiah Couch, Miao Li, Rima Arnaout, Ramy Arnaout

arXiv:2505.22813v1 Announce Type: new 
Abstract: In the universal quest to optimize machine-learning classifiers, three factors -- model architecture, dataset size, and class balance -- have been show...

[📄 Full Paper](https://arxiv.org/abs/2505.22813) | [📝 Analysis](b8b0096633979675dcd26f3ee8774d4f.md)

---

### Equivariant Spherical Transformer for Efficient Molecular Modeling

**Score:** 80.0 | **Published:** 2025-05-31 | **Authors:** Junyi An, Xinyu Lu, Chao Qu, Yunfei Shi, Peijia Lin, Qianwei Tang, Licheng Xu, Fenglei Cao, Yuan Qi

arXiv:2505.23086v1 Announce Type: new 
Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced molecular system modeling by employing group representations. However, their...

[📄 Full Paper](https://arxiv.org/abs/2505.23086) | [📝 Analysis](d9abdebb32b1ade6659d49387ef9d887.md)

---

### The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text

**Score:** 49.1 | **Published:** 2025-05-30 | **Authors:** Maged S. Al-Shaibani, Moataz Ahmed

arXiv:2505.23276v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in generating human-like text, posing subtle yet significant challenges for infor...

[📄 Full Paper](https://arxiv.org/abs/2505.23276) | [📝 Analysis](b855778cc1e57e6b232456f290cac7d9.md)

---

### LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding

**Score:** 45.5 | **Published:** 2025-05-30 | **Authors:** Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen

arXiv:2505.16983v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive ...

[📄 Full Paper](https://arxiv.org/abs/2505.16983) | [📝 Analysis](d38b20877b20ec0877451767892bc8dc.md)

---

### KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction

**Score:** 44.6 | **Published:** 2025-05-30 | **Authors:** Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song

arXiv:2505.23416v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand...

[📄 Full Paper](https://arxiv.org/abs/2505.23416) | [📝 Analysis](355ba89a3f0c38eba4c59e1d6511f1d7.md)

---

### Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs

**Score:** 44.2 | **Published:** 2025-05-30 | **Authors:** Haokun Chen, Yueqi Zhang, Yuan Bi, Yao Zhang, Tong Liu, Jinhe Bi, Jian Lan, Jindong Gu, Claudia Grosser, Denis Krompass, Nassir Navab, Volker Tresp

arXiv:2505.23270v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their...

[📄 Full Paper](https://arxiv.org/abs/2505.23270) | [📝 Analysis](1b390a98d820309b6afbd79cc847a13d.md)

---

### How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias

**Score:** 40.6 | **Published:** 2025-05-30 | **Authors:** Ruiquan Huang, Yingbin Liang, Jing Yang

arXiv:2505.00926v3 Announce Type: replace-cross 
Abstract: Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of lar...

[📄 Full Paper](https://arxiv.org/abs/2505.00926) | [📝 Analysis](63063fee022c60b0bf24b0b022029b1b.md)

---

### GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers

**Score:** 40.2 | **Published:** 2025-05-30 | **Authors:** Guoguo Ai, Guansong Pang, Hezhe Qiao, Yuan Gao, Hui Yan

arXiv:2411.17296v3 Announce Type: replace-cross 
Abstract: Graph Transformers (GTs) have demonstrated remarkable performance in graph representation learning over popular graph neural networks (GNNs)....

[📄 Full Paper](https://arxiv.org/abs/2411.17296) | [📝 Analysis](936c7e26a861cf5b6b3fa112f77ea756.md)

---

### Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms

**Score:** 39.8 | **Published:** 2025-05-30 | **Authors:** Hiroshi Kera, Nico Pelleriti, Yuki Ishihara, Max Zimmer, Sebastian Pokutta

arXiv:2505.23696v1 Announce Type: new 
Abstract: Solving systems of polynomial equations, particularly those with finitely many solutions, is a crucial challenge across many scientific fields. Traditi...

[📄 Full Paper](https://arxiv.org/abs/2505.23696) | [📝 Analysis](09aca18e4dab5c732feff2bc70889178.md)

---

### REOrdering Patches Improves Vision Models

**Score:** 39.8 | **Published:** 2025-05-30 | **Authors:** Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta

arXiv:2505.23751v1 Announce Type: cross 
Abstract: Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening ima...

[📄 Full Paper](https://arxiv.org/abs/2505.23751) | [📝 Analysis](8f372b4d0b07540b80bd028051801e7f.md)

---

### Personality-Guided Code Generation Using Large Language Models

**Score:** 39.2 | **Published:** 2025-05-30 | **Authors:** Yaoqi Guo, Zhenpeng Chen, Jie M. Zhang, Yang Liu, Yun Ma

arXiv:2411.00006v2 Announce Type: replace-cross 
Abstract: Code generation, the automatic creation of source code from natural language descriptions, has garnered significant attention due to its pote...

[📄 Full Paper](https://arxiv.org/abs/2411.00006) | [📝 Analysis](04200f843d0cda9ada7dde59c5f81488.md)

---

### AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity

**Score:** 39.1 | **Published:** 2025-05-30 | **Authors:** Yu Zhang, Dong Guo, Fang Wu, Guoliang Zhu, Dian Ding, Yiming Zhang

arXiv:2505.23520v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with extended context lengths face significant computational challenges during the pre-filling phase, primarily due to the...

[📄 Full Paper](https://arxiv.org/abs/2505.23520) | [📝 Analysis](d99d24daaf1f9912919e1e855d821be3.md)

---

### Tensor Product Attention Is All You Need

**Score:** 38.4 | **Published:** 2025-05-30 | **Authors:** Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew C Yao

arXiv:2501.06425v4 Announce Type: replace 
Abstract: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhe...

[📄 Full Paper](https://arxiv.org/abs/2501.06425) | [📝 Analysis](c74e875e64bea325c07fcd12d29ead49.md)

---

### Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion

**Score:** 37.8 | **Published:** 2025-05-30 | **Authors:** Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li

arXiv:2502.00264v2 Announce Type: replace 
Abstract: Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example i...

[📄 Full Paper](https://arxiv.org/abs/2502.00264) | [📝 Analysis](d37098dedde8670907bdd4de84e61afd.md)

---

### Improving Multilingual Social Media Insights: Aspect-based Comment Analysis

**Score:** 37.1 | **Published:** 2025-05-30 | **Authors:** Longyin Zhang, Bowei Zou, Ai Ti Aw

arXiv:2505.23037v1 Announce Type: new 
Abstract: The inherent nature of social media posts, characterized by the freedom of language use with a disjointed array of diverse opinions and topics, poses s...

[📄 Full Paper](https://arxiv.org/abs/2505.23037) | [📝 Analysis](95b114f69718f6bf63b550ffa41db1bb.md)

---

### A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints

**Score:** 37.0 | **Published:** 2025-05-30 | **Authors:** Michael Munn, Susan Wei

arXiv:2410.05612v2 Announce Type: replace 
Abstract: Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and Vision Transformers....

[📄 Full Paper](https://arxiv.org/abs/2410.05612) | [📝 Analysis](949539f7eaa9816b91ecd8af8521980c.md)

---

### Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability

**Score:** 36.7 | **Published:** 2025-05-30 | **Authors:** Ruida Wang (May), Yuxin Li (May), Yi R. (May),  Fung, Tong Zhang

arXiv:2505.23703v1 Announce Type: new 
Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities....

[📄 Full Paper](https://arxiv.org/abs/2505.23703) | [📝 Analysis](f4f260a36f5e4033fc5b8849c012cc07.md)

---

### Exploring the Limitations of Mamba in COPY and CoT Reasoning

**Score:** 36.6 | **Published:** 2025-05-30 | **Authors:** Ruifeng Ren, Zhicong Li, Yong Liu

arXiv:2410.03810v3 Announce Type: replace-cross 
Abstract: Transformers have become the backbone of modern Large Language Models (LLMs); however, their inference overhead grows linearly with the seque...

[📄 Full Paper](https://arxiv.org/abs/2410.03810) | [📝 Analysis](6196614c151bc8e4636b2a34177e58d3.md)

---

### Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs

**Score:** 36.4 | **Published:** 2025-05-30 | **Authors:** Ngeyen Yinkfu

arXiv:2505.22937v1 Announce Type: new 
Abstract: This study presents an efficient transformer-based question-answering (QA) model optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the S...

[📄 Full Paper](https://arxiv.org/abs/2505.22937) | [📝 Analysis](572bab2f9128491e9e4a177a5c1c05d3.md)

---

### Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation

**Score:** 36.4 | **Published:** 2025-05-30 | **Authors:** Arthur S. Bianchessi, Rodrigo C. Barros, Lucas S. Kupssinsk\"u

arXiv:2505.22842v1 Announce Type: new 
Abstract: Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE...

[📄 Full Paper](https://arxiv.org/abs/2505.22842) | [📝 Analysis](7d72b16152783e8b8a83ed34eabe1e0b.md)

---

### Quartet: Native FP4 Training Can Be Optimal for Large Language Models

**Score:** 35.8 | **Published:** 2025-05-30 | **Authors:** Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh

arXiv:2505.14669v2 Announce Type: replace 
Abstract: Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and...

[📄 Full Paper](https://arxiv.org/abs/2505.14669) | [📝 Analysis](07b0527f0bbac8c63510ac58503a4881.md)

---

### Exploring Scaling Laws for EHR Foundation Models

**Score:** 35.6 | **Published:** 2025-05-30 | **Authors:** Sheng Zhang, Qin Liu, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon

arXiv:2505.22964v1 Announce Type: new 
Abstract: The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through sys...

[📄 Full Paper](https://arxiv.org/abs/2505.22964) | [📝 Analysis](c1ee2cb9aff00f39d8a9db38e1a3d4a8.md)

---

### CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification

**Score:** 35.5 | **Published:** 2025-05-30 | **Authors:** Nawar Turk, Eeham Khan, Leila Kosseim

arXiv:2505.23538v1 Announce Type: new 
Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval), which focuses on verifying promises in corporate ESG (Environmental, Social,...

[📄 Full Paper](https://arxiv.org/abs/2505.23538) | [📝 Analysis](b695d69d039411bf0407628d2491642e.md)

---

### Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning

**Score:** 35.2 | **Published:** 2025-05-30 | **Authors:** Hao Dong, Ziyue Qiao, Zhiyuan Ning, Qi Hao, Yi Du, Pengyang Wang, Yuanchun Zhou

arXiv:2505.14020v2 Announce Type: replace 
Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of k...

[📄 Full Paper](https://arxiv.org/abs/2505.14020) | [📝 Analysis](ef01f9d6019e4233207f399a604f9237.md)

---


*Last updated: 2025-05-31 09:25:12 UTC*
