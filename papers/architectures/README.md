# Architectures Papers

This directory contains papers related to architectures in large language models and AI.

## Papers (25 total)

### Detection of Suicidal Risk on Social Media: A Hybrid Model

**Score:** 92.0 | **Published:** 2025-06-02 | **Authors:** Zaihan Yang, Ryan Leonard, Hien Tran, Rory Driscoll, Chadbourne Davis

arXiv:2505.23797v1 Announce Type: new 
Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable ...

[📄 Full Paper](https://arxiv.org/abs/2505.23797) | [📝 Analysis](12ca4ac0dfb1c635b7e7fd0debd028f0.md)

---

### Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models

**Score:** 92.0 | **Published:** 2025-06-02 | **Authors:** Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir

arXiv:2505.24187v1 Announce Type: new 
Abstract: The prevailing assumption of an exponential decay in large language model (LLM) reliability with sequence length, predicated on independent per-token e...

[📄 Full Paper](https://arxiv.org/abs/2505.24187) | [📝 Analysis](7e452430ec15eccb3737c19a03a40a90.md)

---

### The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It

**Score:** 90.0 | **Published:** 2025-06-02 | **Authors:** Zheng-Xin Yong, Beyza Ermis, Marzieh Fadaee, Stephen H. Bach, Julia Kreutzer

arXiv:2505.24119v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. ...

[📄 Full Paper](https://arxiv.org/abs/2505.24119) | [📝 Analysis](81976d3b9a84f60c643565ef6b6fe6af.md)

---

### The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text

**Score:** 49.1 | **Published:** 2025-05-30 | **Authors:** Maged S. Al-Shaibani, Moataz Ahmed

arXiv:2505.23276v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in generating human-like text, posing subtle yet significant challenges for infor...

[📄 Full Paper](https://arxiv.org/abs/2505.23276) | [📝 Analysis](b855778cc1e57e6b232456f290cac7d9.md)

---

### LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding

**Score:** 45.5 | **Published:** 2025-05-30 | **Authors:** Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen

arXiv:2505.16983v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive ...

[📄 Full Paper](https://arxiv.org/abs/2505.16983) | [📝 Analysis](d38b20877b20ec0877451767892bc8dc.md)

---

### KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction

**Score:** 44.6 | **Published:** 2025-05-30 | **Authors:** Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song

arXiv:2505.23416v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand...

[📄 Full Paper](https://arxiv.org/abs/2505.23416) | [📝 Analysis](355ba89a3f0c38eba4c59e1d6511f1d7.md)

---

### Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs

**Score:** 44.2 | **Published:** 2025-05-30 | **Authors:** Haokun Chen, Yueqi Zhang, Yuan Bi, Yao Zhang, Tong Liu, Jinhe Bi, Jian Lan, Jindong Gu, Claudia Grosser, Denis Krompass, Nassir Navab, Volker Tresp

arXiv:2505.23270v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their...

[📄 Full Paper](https://arxiv.org/abs/2505.23270) | [📝 Analysis](1b390a98d820309b6afbd79cc847a13d.md)

---

### How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias

**Score:** 40.6 | **Published:** 2025-05-30 | **Authors:** Ruiquan Huang, Yingbin Liang, Jing Yang

arXiv:2505.00926v3 Announce Type: replace-cross 
Abstract: Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of lar...

[📄 Full Paper](https://arxiv.org/abs/2505.00926) | [📝 Analysis](63063fee022c60b0bf24b0b022029b1b.md)

---

### GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers

**Score:** 40.2 | **Published:** 2025-05-30 | **Authors:** Guoguo Ai, Guansong Pang, Hezhe Qiao, Yuan Gao, Hui Yan

arXiv:2411.17296v3 Announce Type: replace-cross 
Abstract: Graph Transformers (GTs) have demonstrated remarkable performance in graph representation learning over popular graph neural networks (GNNs)....

[📄 Full Paper](https://arxiv.org/abs/2411.17296) | [📝 Analysis](936c7e26a861cf5b6b3fa112f77ea756.md)

---

### Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms

**Score:** 39.8 | **Published:** 2025-05-30 | **Authors:** Hiroshi Kera, Nico Pelleriti, Yuki Ishihara, Max Zimmer, Sebastian Pokutta

arXiv:2505.23696v1 Announce Type: new 
Abstract: Solving systems of polynomial equations, particularly those with finitely many solutions, is a crucial challenge across many scientific fields. Traditi...

[📄 Full Paper](https://arxiv.org/abs/2505.23696) | [📝 Analysis](09aca18e4dab5c732feff2bc70889178.md)

---

### REOrdering Patches Improves Vision Models

**Score:** 39.8 | **Published:** 2025-05-30 | **Authors:** Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta

arXiv:2505.23751v1 Announce Type: cross 
Abstract: Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening ima...

[📄 Full Paper](https://arxiv.org/abs/2505.23751) | [📝 Analysis](8f372b4d0b07540b80bd028051801e7f.md)

---

### Personality-Guided Code Generation Using Large Language Models

**Score:** 39.2 | **Published:** 2025-05-30 | **Authors:** Yaoqi Guo, Zhenpeng Chen, Jie M. Zhang, Yang Liu, Yun Ma

arXiv:2411.00006v2 Announce Type: replace-cross 
Abstract: Code generation, the automatic creation of source code from natural language descriptions, has garnered significant attention due to its pote...

[📄 Full Paper](https://arxiv.org/abs/2411.00006) | [📝 Analysis](04200f843d0cda9ada7dde59c5f81488.md)

---

### AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity

**Score:** 39.1 | **Published:** 2025-05-30 | **Authors:** Yu Zhang, Dong Guo, Fang Wu, Guoliang Zhu, Dian Ding, Yiming Zhang

arXiv:2505.23520v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with extended context lengths face significant computational challenges during the pre-filling phase, primarily due to the...

[📄 Full Paper](https://arxiv.org/abs/2505.23520) | [📝 Analysis](d99d24daaf1f9912919e1e855d821be3.md)

---

### Tensor Product Attention Is All You Need

**Score:** 38.4 | **Published:** 2025-05-30 | **Authors:** Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew C Yao

arXiv:2501.06425v4 Announce Type: replace 
Abstract: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhe...

[📄 Full Paper](https://arxiv.org/abs/2501.06425) | [📝 Analysis](c74e875e64bea325c07fcd12d29ead49.md)

---

### Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion

**Score:** 37.8 | **Published:** 2025-05-30 | **Authors:** Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li

arXiv:2502.00264v2 Announce Type: replace 
Abstract: Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example i...

[📄 Full Paper](https://arxiv.org/abs/2502.00264) | [📝 Analysis](d37098dedde8670907bdd4de84e61afd.md)

---

### Improving Multilingual Social Media Insights: Aspect-based Comment Analysis

**Score:** 37.1 | **Published:** 2025-05-30 | **Authors:** Longyin Zhang, Bowei Zou, Ai Ti Aw

arXiv:2505.23037v1 Announce Type: new 
Abstract: The inherent nature of social media posts, characterized by the freedom of language use with a disjointed array of diverse opinions and topics, poses s...

[📄 Full Paper](https://arxiv.org/abs/2505.23037) | [📝 Analysis](95b114f69718f6bf63b550ffa41db1bb.md)

---

### A Bayesian Model Selection Criterion for Selecting Pretraining Checkpoints

**Score:** 37.0 | **Published:** 2025-05-30 | **Authors:** Michael Munn, Susan Wei

arXiv:2410.05612v2 Announce Type: replace 
Abstract: Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and Vision Transformers....

[📄 Full Paper](https://arxiv.org/abs/2410.05612) | [📝 Analysis](949539f7eaa9816b91ecd8af8521980c.md)

---

### Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability

**Score:** 36.7 | **Published:** 2025-05-30 | **Authors:** Ruida Wang (May), Yuxin Li (May), Yi R. (May),  Fung, Tong Zhang

arXiv:2505.23703v1 Announce Type: new 
Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities....

[📄 Full Paper](https://arxiv.org/abs/2505.23703) | [📝 Analysis](f4f260a36f5e4033fc5b8849c012cc07.md)

---

### Exploring the Limitations of Mamba in COPY and CoT Reasoning

**Score:** 36.6 | **Published:** 2025-05-30 | **Authors:** Ruifeng Ren, Zhicong Li, Yong Liu

arXiv:2410.03810v3 Announce Type: replace-cross 
Abstract: Transformers have become the backbone of modern Large Language Models (LLMs); however, their inference overhead grows linearly with the seque...

[📄 Full Paper](https://arxiv.org/abs/2410.03810) | [📝 Analysis](6196614c151bc8e4636b2a34177e58d3.md)

---

### Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs

**Score:** 36.4 | **Published:** 2025-05-30 | **Authors:** Ngeyen Yinkfu

arXiv:2505.22937v1 Announce Type: new 
Abstract: This study presents an efficient transformer-based question-answering (QA) model optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the S...

[📄 Full Paper](https://arxiv.org/abs/2505.22937) | [📝 Analysis](572bab2f9128491e9e4a177a5c1c05d3.md)

---

### Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation

**Score:** 36.4 | **Published:** 2025-05-30 | **Authors:** Arthur S. Bianchessi, Rodrigo C. Barros, Lucas S. Kupssinsk\"u

arXiv:2505.22842v1 Announce Type: new 
Abstract: Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE...

[📄 Full Paper](https://arxiv.org/abs/2505.22842) | [📝 Analysis](7d72b16152783e8b8a83ed34eabe1e0b.md)

---

### Quartet: Native FP4 Training Can Be Optimal for Large Language Models

**Score:** 35.8 | **Published:** 2025-05-30 | **Authors:** Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh

arXiv:2505.14669v2 Announce Type: replace 
Abstract: Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and...

[📄 Full Paper](https://arxiv.org/abs/2505.14669) | [📝 Analysis](07b0527f0bbac8c63510ac58503a4881.md)

---

### Exploring Scaling Laws for EHR Foundation Models

**Score:** 35.6 | **Published:** 2025-05-30 | **Authors:** Sheng Zhang, Qin Liu, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon

arXiv:2505.22964v1 Announce Type: new 
Abstract: The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through sys...

[📄 Full Paper](https://arxiv.org/abs/2505.22964) | [📝 Analysis](c1ee2cb9aff00f39d8a9db38e1a3d4a8.md)

---

### CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification

**Score:** 35.5 | **Published:** 2025-05-30 | **Authors:** Nawar Turk, Eeham Khan, Leila Kosseim

arXiv:2505.23538v1 Announce Type: new 
Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval), which focuses on verifying promises in corporate ESG (Environmental, Social,...

[📄 Full Paper](https://arxiv.org/abs/2505.23538) | [📝 Analysis](b695d69d039411bf0407628d2491642e.md)

---

### Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning

**Score:** 35.2 | **Published:** 2025-05-30 | **Authors:** Hao Dong, Ziyue Qiao, Zhiyuan Ning, Qi Hao, Yi Du, Pengyang Wang, Yuanchun Zhou

arXiv:2505.14020v2 Announce Type: replace 
Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of k...

[📄 Full Paper](https://arxiv.org/abs/2505.14020) | [📝 Analysis](ef01f9d6019e4233207f399a604f9237.md)

---


*Last updated: 2025-06-02 09:29:50 UTC*
