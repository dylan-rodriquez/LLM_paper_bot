# GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers

**Authors:** Guoguo Ai, Guansong Pang, Hezhe Qiao, Yuan Gao, Hui Yan

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 40.2/100

## Abstract

arXiv:2411.17296v3 Announce Type: replace-cross 
Abstract: Graph Transformers (GTs) have demonstrated remarkable performance in graph representation learning over popular graph neural networks (GNNs). However, self--attention, the core module of GTs, preserves only low-frequency signals in graph features, leading to ineffectiveness in capturing other important signals like high-frequency ones. Some recent GT models help alleviate this issue, but their flexibility and expressiveness are still limited since the filters they learn are fixed on predefined graph spectrum or spectral order. To tackle this challenge, we propose a Graph Fourier Kolmogorov-Arnold Transformer (GrokFormer), a novel GT model that learns highly expressive spectral filters with adaptive graph spectrum and spectral order through a Fourier series modeling over learnable activation functions. We demonstrate theoretically and empirically that the proposed GrokFormer filter offers better expressiveness than other spectral methods. Comprehensive experiments on 10 real-world node classification datasets across various domains, scales, and graph properties, as well as 5 graph classification datasets, show that GrokFormer outperforms state-of-the-art GTs and GNNs. Our code is available at https://github.com/GGA23/GrokFormer

## Analysis

**Innovation Score:** 30.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 57.1/100

**Justification:** High innovation indicators (score: 30); Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)

## Keywords

graph, grokformer, spectral, fourier, gts, arnold, classification, classification datasets, datasets, expressiveness

## Links

- [Paper URL](https://arxiv.org/abs/2411.17296)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
