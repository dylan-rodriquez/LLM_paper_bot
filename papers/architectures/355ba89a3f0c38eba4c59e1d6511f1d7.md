# KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction

**Authors:** Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.DB

**Significance Score:** 44.6/100

## Abstract

arXiv:2505.23416v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.

## Analysis

**Innovation Score:** 20.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 50.3/100

**Justification:** Strong impact potential (score: 24); Contains key LLM terms (bonus: 20)

## Keywords

kv, cache, kvzip, context, kv cache, query, pairs, agnostic, agnostic kv, evaluations

## Links

- [Paper URL](https://arxiv.org/abs/2505.23416)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
