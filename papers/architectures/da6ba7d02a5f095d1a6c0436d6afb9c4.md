# Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery

**Authors:** Ning Liu, Yue Yu

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.23106v1 Announce Type: new 
Abstract: Attention mechanisms have emerged as transformative tools in core AI domains such as natural language processing and computer vision. Yet, their largely untapped potential for modeling intricate physical systems presents a compelling frontier. Learning such systems often entails discovering operators that map between functional spaces using limited instances of function pairs -- a task commonly framed as a severely ill-posed inverse PDE problem. In this work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator architecture that builds upon and enhances Nonlocal Attention Operators (NAO) in both predictive accuracy and computational efficiency. NIPS employs a linear attention mechanism to enable scalable learning and integrates a learnable kernel network that acts as a channel-independent convolution in Fourier space. As a consequence, NIPS eliminates the need to explicitly compute and store large pairwise interactions, effectively amortizing the cost of handling spatial interactions into the Fourier transform. Empirical evaluations demonstrate that NIPS consistently surpasses NAO and other baselines across diverse benchmarks, heralding a substantial leap in scalable, interpretable, and efficient physics learning. Our code and data accompanying this paper are available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 70.0/100  
**Sentiment Score:** 80.0/100

**Justification:** The paper addresses a significant problem – scalable and interpretable physics discovery – using a novel neural operator architecture. The combination of linear attention and a learnable kernel network in Fourier space appears promising for improving both accuracy and efficiency. While the abstract doesn't detail experimental results, the described approach seems well-motivated and builds upon existing work (NAO) in a logical manner, suggesting a solid methodology. The potential for eliminating large pairwise interaction computations is a key benefit.

## Keywords

attention, interpretable, nips, fourier, learning, neural, scalable, interactions, interpretable pdes, nao

## Links

- [Paper URL](https://arxiv.org/abs/2505.23106)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
