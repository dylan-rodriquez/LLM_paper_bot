# Update Your Transformer to the Latest Release: Re-Basin of Task Vectors

**Authors:** Filippo Rinaldi, Giacomo Capitani, Lorenzo Bonicelli, Donato Crisostomi, Federico Bolelli, Elisa Ficarra, Emanuele Rodol\`a, Simone Calderara, Angelo Porrello

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.22697v1 Announce Type: new 
Abstract: Foundation models serve as the backbone for numerous specialized models developed through fine-tuning. However, when the underlying pretrained model is updated or retrained (e.g., on larger and more curated datasets), the fine-tuned model becomes obsolete, losing its utility and requiring retraining. This raises the question: is it possible to transfer fine-tuning to a new release of the model? In this work, we investigate how to transfer fine-tuning to a new checkpoint without having to re-train, in a data-free manner. To do so, we draw principles from model re-basin and provide a recipe based on weight permutations to re-base the modifications made to the original base model, often called task vector. In particular, our approach tailors model re-basin for Transformer models, taking into account the challenges of residual connections and multi-head attention layers. Specifically, we propose a two-level method rooted in spectral theory, initially permuting the attention heads and subsequently adjusting parameters within select pairs of heads. Through extensive experiments on visual and textual tasks, we achieve the seamless transfer of fine-tuned knowledge to new pre-trained backbones without relying on a single training step or datapoint. Code is available at https://github.com/aimagelab/TransFusion.

## Analysis

**Innovation Score:** 72.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 82.0/100

**Justification:** This paper addresses a very practical and important problem in the rapidly evolving landscape of foundation models â€“ the cost of retraining fine-tuned models with each base model update. The approach of adapting 'model re-basin' principles for Transformers, particularly with attention to residual connections and multi-head attention, shows a thoughtful methodology. While the abstract hints at a complex, spectral-theory-based solution, the data-free transfer aspect is particularly appealing, suggesting potential for significant efficiency gains. The problem is well-defined and the proposed solution appears promising, though the details will be crucial for full assessment.

## Keywords

model, fine, new, basin, fine tuning, models, transfer, transfer fine, tuning, attention

## Links

- [Paper URL](https://arxiv.org/abs/2505.22697)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
