# Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation

**Authors:** Arthur S. Bianchessi, Rodrigo C. Barros, Lucas S. Kupssinsk\"u

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 36.4/100

## Abstract

arXiv:2505.22842v1 Announce Type: new 
Abstract: Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods lack theoretical clarity and rely on limited evaluation metrics to substantiate their extrapolation claims. We propose the Bayesian Attention Mechanism (BAM), a theoretical framework that formulates positional encoding as a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE and ALiBi) and motivates a new Generalized Gaussian positional prior that substantially improves long-context generalization. Empirically, BAM enables accurate information retrieval at $500\times$ the training context length, outperforming previous state-of-the-art context length generalization in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.

## Analysis

**Innovation Score:** 30.0/100
**Impact Score:** 8.0/100  
**Sentiment Score:** 51.8/100

**Justification:** High innovation indicators (score: 30); Contains key LLM terms (bonus: 10)

## Keywords

context, context length, length, positional, bam, encoding, extrapolation, positional encoding, attention, attention mechanism

## Links

- [Paper URL](https://arxiv.org/abs/2505.22842)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
