# Quartet: Native FP4 Training Can Be Optimal for Large Language Models

**Authors:** Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 35.8/100

## Abstract

arXiv:2505.14669v2 Announce Type: replace 
Abstract: Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an "optimal" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.

## Analysis

**Innovation Score:** 10.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 52.5/100

**Justification:** Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)

## Keywords

training, precision, fp4, low, low precision, models, quartet, fp4 training, accuracy, blackwell

## Links

- [Paper URL](https://arxiv.org/abs/2505.14669)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
