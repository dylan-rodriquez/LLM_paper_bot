{
  "papers": [
    {
      "title": "ExpeTrans: LLMs Are Experiential Transfer Learners",
      "authors": [
        "Jinglong Gao, Xiao Ding, Lingxiao Zou, Bibo Cai, Bing Qin, Ting Liu"
      ],
      "abstract": "arXiv:2505.23191v1 Announce Type: new \nAbstract: Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance. However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs. To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs. Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.",
      "url": "https://arxiv.org/abs/2505.23191",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 41.17,
      "innovation_score": 30,
      "impact_score": 24,
      "sentiment_score": 55.85,
      "keywords": [
        "llms",
        "experience",
        "framework",
        "task",
        "transfer",
        "experiences",
        "human",
        "methods",
        "performance",
        "previous"
      ],
      "subject_classification": "theoretical",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)",
      "paper_id": "a5e0460b6c801f0c8aa3b7da9fb44bfd"
    },
    {
      "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
      "authors": [
        "Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang"
      ],
      "abstract": "arXiv:2505.03475v2 Announce Type: replace \nAbstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.",
      "url": "https://arxiv.org/abs/2505.03475",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.AI"
      ],
      "significance_score": 39.94,
      "innovation_score": 10,
      "impact_score": 16,
      "sentiment_score": 52.2,
      "keywords": [
        "elo",
        "evaluation",
        "framework",
        "arena",
        "based",
        "elo rating",
        "method",
        "rating",
        "stable",
        "abilities"
      ],
      "subject_classification": "theoretical",
      "justification": "Contains key LLM terms (bonus: 15)",
      "paper_id": "70bdaebd4bb2cafc80ef22175d16c14f"
    },
    {
      "title": "Autoformalization in the Era of Large Language Models: A Survey",
      "authors": [
        "Ke Weng, Lun Du, Sirui Li, Wangyue Lu, Haozhe Sun, Hengyu Liu, Tiancheng Zhang"
      ],
      "abstract": "arXiv:2505.23486v1 Announce Type: new \nAbstract: Autoformalization, the process of transforming informal mathematical propositions into verifiable formal representations, is a foundational task in automated theorem proving, offering a new perspective on the use of mathematics in both theoretical and applied domains. Driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), this field has witnessed substantial growth, bringing both new opportunities and unique challenges. In this survey, we provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives. We examine how autoformalization is applied across various mathematical domains and levels of difficulty, and analyze the end-to-end workflow from data preprocessing to model design and evaluation. We further explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs, highlighting its potential to improve both the trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.",
      "url": "https://arxiv.org/abs/2505.23486",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.AI"
      ],
      "significance_score": 35.24,
      "innovation_score": 20,
      "impact_score": 16,
      "sentiment_score": 57.45,
      "keywords": [
        "autoformalization",
        "mathematical",
        "models",
        "new",
        "applied",
        "challenges",
        "domains",
        "end",
        "field",
        "language"
      ],
      "subject_classification": "theoretical",
      "justification": "Contains key LLM terms (bonus: 10)",
      "paper_id": "4be2659b76eaf5d4556519e0d4a6ae93"
    },
    {
      "title": "GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification",
      "authors": [
        "Iknoor Singh, Carolina Scarton, Kalina Bontcheva"
      ],
      "abstract": "arXiv:2505.22867v1 Announce Type: new \nAbstract: The proliferation of online news and the increasing spread of misinformation necessitate robust methods for automatic data analysis. Narrative classification is emerging as a important task, since identifying what is being said online is critical for fact-checkers, policy markers and other professionals working on information studies. This paper presents our approach to SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a pre-defined two-level taxonomy of main narratives and sub-narratives across multiple languages.\n  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual narrative classification. Our methodology follows a three-step Large Language Model (LLM) prompting strategy, where the model first categorises an article into one of two domains (Ukraine-Russia War or Climate Change), then identifies the most relevant main narratives, and finally assigns sub-narratives. Our approach secured the top position on the English test set among 28 competing teams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.",
      "url": "https://arxiv.org/abs/2505.22867",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 35.01,
      "innovation_score": 20,
      "impact_score": 16,
      "sentiment_score": 50.05,
      "keywords": [
        "narratives",
        "classification",
        "narrative",
        "narrative classification",
        "prompting",
        "step",
        "task",
        "10",
        "2025",
        "2025 task"
      ],
      "subject_classification": "theoretical",
      "justification": "Contains key LLM terms (bonus: 10)",
      "paper_id": "2292fde3dfa1661b010cba5193fadbbe"
    },
    {
      "title": "Multilook Coherent Imaging: Theoretical Guarantees and Algorithms",
      "authors": [
        "Xi Chen, Soham Jana, Christopher A. Metzler, Arian Maleki, Shirin Jalali"
      ],
      "abstract": "arXiv:2505.23594v1 Announce Type: cross \nAbstract: Multilook coherent imaging is a widely used technique in applications such as digital holography, ultrasound imaging, and synthetic aperture radar. A central challenge in these systems is the presence of multiplicative noise, commonly known as speckle, which degrades image quality. Despite the widespread use of coherent imaging systems, their theoretical foundations remain relatively underexplored. In this paper, we study both the theoretical and algorithmic aspects of likelihood-based approaches for multilook coherent imaging, providing a rigorous framework for analysis and method development. Our theoretical contributions include establishing the first theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our results capture the dependence of MSE on the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we employ projected gradient descent (PGD) as an efficient method for computing the maximum likelihood solution. Furthermore, we introduce two key ideas to enhance the practical performance of PGD. First, we incorporate the Newton-Schulz algorithm to compute matrix inverses within the PGD iterations, significantly reducing computational complexity. Second, we develop a bagging strategy to mitigate projection errors introduced during PGD updates. We demonstrate that combining these techniques with PGD yields state-of-the-art performance. Our code is available at https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.",
      "url": "https://arxiv.org/abs/2505.23594",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "stat.ML"
      ],
      "significance_score": 35.0,
      "innovation_score": 30,
      "impact_score": 24,
      "sentiment_score": 50.0,
      "keywords": [
        "imaging",
        "pgd",
        "theoretical",
        "coherent",
        "coherent imaging",
        "image",
        "likelihood",
        "multilook",
        "multilook coherent",
        "number"
      ],
      "subject_classification": "theoretical",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 24); Technical sophistication (score: 40)",
      "paper_id": "fe9862435743e7b42c4ea6b60fab74ca"
    }
  ],
  "last_updated": "2025-05-30T11:01:12.488627"
}