{
  "papers": [
    {
      "title": "ExpeTrans: LLMs Are Experiential Transfer Learners",
      "authors": [
        "Jinglong Gao, Xiao Ding, Lingxiao Zou, Bibo Cai, Bing Qin, Ting Liu"
      ],
      "abstract": "arXiv:2505.23191v1 Announce Type: new \nAbstract: Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance. However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs. To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs. Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.",
      "url": "https://arxiv.org/abs/2505.23191",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 41.17,
      "innovation_score": 30,
      "impact_score": 24,
      "sentiment_score": 55.85,
      "keywords": [
        "llms",
        "experience",
        "framework",
        "task",
        "transfer",
        "experiences",
        "human",
        "methods",
        "performance",
        "previous"
      ],
      "subject_classification": "theoretical",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)",
      "paper_id": "a5e0460b6c801f0c8aa3b7da9fb44bfd"
    },
    {
      "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
      "authors": [
        "Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang"
      ],
      "abstract": "arXiv:2505.03475v2 Announce Type: replace \nAbstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.",
      "url": "https://arxiv.org/abs/2505.03475",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.AI"
      ],
      "significance_score": 39.94,
      "innovation_score": 10,
      "impact_score": 16,
      "sentiment_score": 52.2,
      "keywords": [
        "elo",
        "evaluation",
        "framework",
        "arena",
        "based",
        "elo rating",
        "method",
        "rating",
        "stable",
        "abilities"
      ],
      "subject_classification": "theoretical",
      "justification": "Contains key LLM terms (bonus: 15)",
      "paper_id": "70bdaebd4bb2cafc80ef22175d16c14f"
    },
    {
      "title": "Autoformalization in the Era of Large Language Models: A Survey",
      "authors": [
        "Ke Weng, Lun Du, Sirui Li, Wangyue Lu, Haozhe Sun, Hengyu Liu, Tiancheng Zhang"
      ],
      "abstract": "arXiv:2505.23486v1 Announce Type: new \nAbstract: Autoformalization, the process of transforming informal mathematical propositions into verifiable formal representations, is a foundational task in automated theorem proving, offering a new perspective on the use of mathematics in both theoretical and applied domains. Driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), this field has witnessed substantial growth, bringing both new opportunities and unique challenges. In this survey, we provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives. We examine how autoformalization is applied across various mathematical domains and levels of difficulty, and analyze the end-to-end workflow from data preprocessing to model design and evaluation. We further explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs, highlighting its potential to improve both the trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.",
      "url": "https://arxiv.org/abs/2505.23486",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.AI"
      ],
      "significance_score": 35.24,
      "innovation_score": 20,
      "impact_score": 16,
      "sentiment_score": 57.45,
      "keywords": [
        "autoformalization",
        "mathematical",
        "models",
        "new",
        "applied",
        "challenges",
        "domains",
        "end",
        "field",
        "language"
      ],
      "subject_classification": "theoretical",
      "justification": "Contains key LLM terms (bonus: 10)",
      "paper_id": "4be2659b76eaf5d4556519e0d4a6ae93"
    },
    {
      "title": "GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification",
      "authors": [
        "Iknoor Singh, Carolina Scarton, Kalina Bontcheva"
      ],
      "abstract": "arXiv:2505.22867v1 Announce Type: new \nAbstract: The proliferation of online news and the increasing spread of misinformation necessitate robust methods for automatic data analysis. Narrative classification is emerging as a important task, since identifying what is being said online is critical for fact-checkers, policy markers and other professionals working on information studies. This paper presents our approach to SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a pre-defined two-level taxonomy of main narratives and sub-narratives across multiple languages.\n  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual narrative classification. Our methodology follows a three-step Large Language Model (LLM) prompting strategy, where the model first categorises an article into one of two domains (Ukraine-Russia War or Climate Change), then identifies the most relevant main narratives, and finally assigns sub-narratives. Our approach secured the top position on the English test set among 28 competing teams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.",
      "url": "https://arxiv.org/abs/2505.22867",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.CL"
      ],
      "significance_score": 35.01,
      "innovation_score": 20,
      "impact_score": 16,
      "sentiment_score": 50.05,
      "keywords": [
        "narratives",
        "classification",
        "narrative",
        "narrative classification",
        "prompting",
        "step",
        "task",
        "10",
        "2025",
        "2025 task"
      ],
      "subject_classification": "theoretical",
      "justification": "Contains key LLM terms (bonus: 10)",
      "paper_id": "2292fde3dfa1661b010cba5193fadbbe"
    },
    {
      "title": "Multilook Coherent Imaging: Theoretical Guarantees and Algorithms",
      "authors": [
        "Xi Chen, Soham Jana, Christopher A. Metzler, Arian Maleki, Shirin Jalali"
      ],
      "abstract": "arXiv:2505.23594v1 Announce Type: cross \nAbstract: Multilook coherent imaging is a widely used technique in applications such as digital holography, ultrasound imaging, and synthetic aperture radar. A central challenge in these systems is the presence of multiplicative noise, commonly known as speckle, which degrades image quality. Despite the widespread use of coherent imaging systems, their theoretical foundations remain relatively underexplored. In this paper, we study both the theoretical and algorithmic aspects of likelihood-based approaches for multilook coherent imaging, providing a rigorous framework for analysis and method development. Our theoretical contributions include establishing the first theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our results capture the dependence of MSE on the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we employ projected gradient descent (PGD) as an efficient method for computing the maximum likelihood solution. Furthermore, we introduce two key ideas to enhance the practical performance of PGD. First, we incorporate the Newton-Schulz algorithm to compute matrix inverses within the PGD iterations, significantly reducing computational complexity. Second, we develop a bagging strategy to mitigate projection errors introduced during PGD updates. We demonstrate that combining these techniques with PGD yields state-of-the-art performance. Our code is available at https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.",
      "url": "https://arxiv.org/abs/2505.23594",
      "published_date": "2025-05-30T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "stat.ML"
      ],
      "significance_score": 35.0,
      "innovation_score": 30,
      "impact_score": 24,
      "sentiment_score": 50.0,
      "keywords": [
        "imaging",
        "pgd",
        "theoretical",
        "coherent",
        "coherent imaging",
        "image",
        "likelihood",
        "multilook",
        "multilook coherent",
        "number"
      ],
      "subject_classification": "theoretical",
      "justification": "High innovation indicators (score: 30); Strong impact potential (score: 24); Technical sophistication (score: 40)",
      "paper_id": "fe9862435743e7b42c4ea6b60fab74ca"
    },
    {
      "title": "Best Arm Identification with Possibly Biased Offline Data",
      "authors": [
        "Le Yang, Vincent Y. F. Tan, Wang Chi Cheung"
      ],
      "abstract": "arXiv:2505.23165v1 Announce Type: new \nAbstract: We study the best arm identification (BAI) problem with potentially biased offline data in the fixed confidence setting, which commonly arises in real-world scenarios such as clinical trials. We prove an impossibility result for adaptive algorithms without prior knowledge of the bias bound between online and offline distributions. To address this, we propose the LUCB-H algorithm, which introduces adaptive confidence bounds by incorporating an auxiliary bias correction to balance offline and online data within the LUCB framework. Theoretical analysis shows that LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and significantly outperforms it when offline data is helpful. We also derive an instance-dependent lower bound that matches the upper bound of LUCB-H in certain scenarios. Numerical experiments further demonstrate the robustness and adaptability of LUCB-H in effectively incorporating offline data.",
      "url": "https://arxiv.org/abs/2505.23165",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 85,
      "innovation_score": 75,
      "impact_score": 78,
      "sentiment_score": 80,
      "keywords": [
        "offline",
        "data",
        "lucb",
        "offline data",
        "bound",
        "adaptive",
        "arm",
        "arm identification",
        "best",
        "best arm"
      ],
      "subject_classification": "theoretical",
      "justification": "This paper tackles a relevant and practical problem \u2013 best arm identification with biased offline data, common in fields like clinical trials. The impossibility result is a strong theoretical contribution, and the LUCB-H algorithm appears to be a well-motivated and theoretically sound approach to address the identified limitations. The matching lower bound and empirical validation further strengthen the work, suggesting a high degree of rigor and potential for positive reception.",
      "paper_id": "ae65e2f83f1d61ea23954ef2976b12b4"
    },
    {
      "title": "When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?",
      "authors": [
        "Eleni Nisioti, Joachim Winther Pedersen, Erwan Plantec, Milton L. Montero, Sebastian Risi"
      ],
      "abstract": "arXiv:2505.22696v1 Announce Type: new \nAbstract: The ability to continuously and efficiently transfer skills across tasks is a hallmark of biological intelligence and a long-standing goal in artificial systems. Reinforcement learning (RL), a dominant paradigm for learning in high-dimensional control tasks, is known to suffer from brittleness to task variations and catastrophic forgetting. Neuroevolution (NE) has recently gained attention for its robustness, scalability, and capacity to escape local optima. In this paper, we investigate an understudied dimension of NE: its transfer learning capabilities. To this end, we introduce two benchmarks: a) in stepping gates, neural networks are tasked with emulating logic circuits, with designs that emphasize modular repetition and variation b) ecorobot extends the Brax physics engine with objects such as walls and obstacles and the ability to easily switch between different robotic morphologies. Crucial in both benchmarks is the presence of a curriculum that enables evaluating skill transfer across tasks of increasing complexity. Our empirical analysis shows that NE methods vary in their transfer abilities and frequently outperform RL baselines. Our findings support the potential of NE as a foundation for building more adaptable agents and highlight future challenges for scaling NE to complex, real-world problems.",
      "url": "https://arxiv.org/abs/2505.22696",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 82,
      "innovation_score": 70,
      "impact_score": 75,
      "sentiment_score": 85,
      "keywords": [
        "learning",
        "ne",
        "transfer",
        "tasks",
        "ability",
        "benchmarks",
        "neuroevolution",
        "reinforcement",
        "reinforcement learning",
        "rl"
      ],
      "subject_classification": "theoretical",
      "justification": "This paper addresses a crucial problem in AI \u2013 the brittleness of RL and the potential of NE for transfer learning. The introduction of two new benchmarks, 'stepping gates' and 'ecorobot', is a strong methodological contribution, providing a valuable resource for the community. While NE is gaining traction, a systematic comparison with RL in transfer learning is relatively underexplored, making this work timely and relevant. The abstract suggests a rigorous investigation, though the full paper would need to be reviewed to confirm.",
      "paper_id": "5bed996815cd0cf35a12431c6793f93c"
    },
    {
      "title": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models",
      "authors": [
        "Jipeng Li, Yanning Shen"
      ],
      "abstract": "arXiv:2505.22935v1 Announce Type: new \nAbstract: Explicit noise-level conditioning is widely regarded as essential for the effective operation of Graph Diffusion Models (GDMs). In this work, we challenge this assumption by investigating whether denoisers can implicitly infer noise levels directly from corrupted graph structures, potentially eliminating the need for explicit noise conditioning. To this end, we develop a theoretical framework centered on Bernoulli edge-flip corruptions and extend it to encompass more complex scenarios involving coupled structure-attribute noise. Extensive empirical evaluations on both synthetic and real-world graph datasets, using models such as GDSS and DiGress, provide strong support for our theoretical findings. Notably, unconditional GDMs achieve performance comparable or superior to their conditioned counterparts, while also offering reductions in parameters (4-6%) and computation time (8-10%). Our results suggest that the high-dimensional nature of graph data itself often encodes sufficient information for the denoising process, opening avenues for simpler, more efficient GDM architectures.",
      "url": "https://arxiv.org/abs/2505.22935",
      "published_date": "2025-05-31T00:00:00",
      "source": "arXiv RSS",
      "categories": [
        "cs.LG"
      ],
      "significance_score": 80,
      "innovation_score": 75,
      "impact_score": 70,
      "sentiment_score": 88,
      "keywords": [
        "graph",
        "noise",
        "conditioning",
        "models",
        "diffusion",
        "diffusion models",
        "explicit",
        "explicit noise",
        "gdms",
        "graph diffusion"
      ],
      "subject_classification": "theoretical",
      "justification": "This paper tackles a fundamental assumption in Graph Diffusion Models (GDMs) \u2013 the necessity of noise conditioning \u2013 and provides both theoretical grounding and empirical evidence to challenge it. The reported improvements in parameter efficiency and computation time are significant, suggesting a practical benefit. The use of both synthetic and real-world datasets strengthens the findings, and the comparison to established models like GDSS and DiGress adds credibility. The high sentiment score reflects the potential for positive reception within the graph ML community.",
      "paper_id": "9dde7589cca979f0170a2bbd7fb58586"
    }
  ],
  "last_updated": "2025-05-31T09:25:12.833821"
}