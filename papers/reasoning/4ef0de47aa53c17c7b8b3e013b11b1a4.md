# Probability-Consistent Preference Optimization for Enhanced LLM Reasoning

**Authors:** Yunqiao Yang, Houxing Ren, Zimu Lu, Ke Wang, Weikang Shi, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 45.2/100

## Abstract

arXiv:2505.23540v1 Announce Type: new 
Abstract: Recent advances in preference optimization have demonstrated significant potential for improving mathematical reasoning capabilities in large language models (LLMs). While current approaches leverage high-quality pairwise preference data through outcome-based criteria like answer correctness or consistency, they fundamentally neglect the internal logical coherence of responses. To overcome this, we propose Probability-Consistent Preference Optimization (PCPO), a novel framework that establishes dual quantitative metrics for preference selection: (1) surface-level answer correctness and (2) intrinsic token-level probability consistency across responses. Extensive experiments show that our PCPO consistently outperforms existing outcome-only criterion approaches across a diverse range of LLMs and benchmarks. Our code is publicly available at https://github.com/YunqiaoYang/PCPO.

## Analysis

**Innovation Score:** 40.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 55.0/100

**Justification:** High innovation indicators (score: 40); Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)

## Keywords

preference, optimization, pcpo, preference optimization, probability, answer, answer correctness, approaches, consistency, consistent

## Links

- [Paper URL](https://arxiv.org/abs/2505.23540)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
