# Scalable Complexity Control Facilitates Reasoning Ability of LLMs

**Authors:** Liangkai Hang, Junjie Yao, Zhiwei Bai, Tianyi Chen, Yang Chen, Rongjie Diao, Hezhou Li, Pengxiao Lin, Zhiwei Wang, Cheng Xu, Zhongwang Zhang, Zhangchen Zhou, Zhiyu Li, Zehao Lin, Kai Chen, Feiyu Xiong, Yaoyu Zhang, Weinan E, Hongkang Yang, Zhi-Qin John Xu

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 82.0/100

## Abstract

arXiv:2505.23013v1 Announce Type: new 
Abstract: The reasoning ability of large language models (LLMs) has been rapidly advancing in recent years, attracting interest in more fundamental approaches that can reliably enhance their generalizability. This work demonstrates that model complexity control, conveniently implementable by adjusting the initialization rate and weight decay coefficient, improves the scaling law of LLMs consistently over varying model sizes and data sizes. This gain is further illustrated by comparing the benchmark performance of 2.4B models pretrained on 1T tokens with different complexity hyperparameters. Instead of fixing the initialization std, we found that a constant initialization rate (the exponent of std) enables the scaling law to descend faster in both model and data sizes. These results indicate that complexity control is a promising direction for the continual advancement of LLMs.

## Analysis

**Innovation Score:** 65.0/100
**Impact Score:** 75.0/100  
**Sentiment Score:** 85.0/100

**Justification:** The paper addresses a crucial area in LLM research â€“ improving scaling laws and reasoning ability. The finding that adjusting initialization rate and weight decay can consistently improve performance across different model and data sizes is promising. While the method itself (adjusting hyperparameters) isn't entirely novel, the consistent improvement and the focus on a 'constant initialization rate' suggest a valuable refinement to existing training practices. The positive sentiment is driven by the current focus on efficient and scalable LLM development.

## Keywords

complexity, llms, complexity control, control, initialization, model, sizes, ability, data, data sizes

## Links

- [Paper URL](https://arxiv.org/abs/2505.23013)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
