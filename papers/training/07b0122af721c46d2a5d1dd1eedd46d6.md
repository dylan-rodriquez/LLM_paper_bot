# Private Rate-Constrained Optimization with Applications to Fair Learning

**Authors:** Mohammad Yaghini, Tudor Cebere, Michael Menart, Aur\'elien Bellet, Nicolas Papernot

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.22703v1 Announce Type: new 
Abstract: Many problems in trustworthy ML can be formulated as minimization of the model error under constraints on the prediction rates of the model for suitably-chosen marginals, including most group fairness constraints (demographic parity, equality of odds, etc.). In this work, we study such constrained minimization problems under differential privacy (DP). Standard DP optimization techniques like DP-SGD rely on the loss function's decomposability into per-sample contributions. However, rate constraints introduce inter-sample dependencies, violating the decomposability requirement. To address this, we develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm which solves the Lagrangian formulation of rate constraint problems. We demonstrate that the additional privacy cost of incorporating these constraints reduces to privately estimating a histogram over the mini-batch at each optimization step. We prove the convergence of our algorithm through a novel analysis of SGDA that leverages the linear structure of the dual parameter. Finally, empirical results on learning under group fairness constraints demonstrate that our method Pareto-dominates existing private learning approaches in fairness-utility trade-offs.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 78.0/100  
**Sentiment Score:** 80.0/100

**Justification:** This paper tackles a crucial problem in trustworthy ML â€“ combining differential privacy with rate constraints, which are essential for fairness. The development of RaCO-DP to address the decomposability issue with standard DP-SGD is a solid contribution. The reduction of the privacy cost to histogram estimation is a clever optimization, and the abstract suggests rigorous proof of convergence. The topic is highly relevant given the increasing focus on fairness and privacy in machine learning.

## Keywords

constraints, dp, fairness, learning, optimization, problems, rate, algorithm, constrained, decomposability

## Links

- [Paper URL](https://arxiv.org/abs/2505.22703)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
