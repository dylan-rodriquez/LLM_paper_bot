# When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text

**Authors:** Hillary Dawkins, Kathleen C. Fraser, Svetlana Kiritchenko

**Published:** 2025-06-12 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 92.0/100

## Abstract

arXiv:2506.09975v1 Announce Type: new 
Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting AI-generated text on social media is made even more difficult due to the short text length and informal, idiosyncratic language of the internet. It is nonetheless important to tackle this problem, as social media represents a significant attack vector in online influence campaigns, which may be bolstered through the use of mass-produced AI-generated posts supporting (or opposing) particular policies, decisions, or events. We approach this problem with the mindset and resources of a reasonably sophisticated threat actor, and create a dataset of 505,159 AI-generated social media posts from a combination of open-source, closed-source, and fine-tuned LLMs, covering 11 different controversial topics. We show that while the posts can be detected under typical research assumptions about knowledge of and access to the generating models, under the more realistic assumption that an attacker will not release their fine-tuned model to the public, detectability drops dramatically. This result is confirmed with a human study. Ablation experiments highlight the vulnerability of various detection algorithms to fine-tuned LLMs. This result has implications across all detection domains, since fine-tuning is a generally applicable and realistic LLM use case.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 88.0/100  
**Sentiment Score:** 85.0/100

**Justification:** This research tackles a highly relevant and increasingly important problem â€“ the detection of AI-generated content on social media, specifically focusing on the challenges posed by short-form, informal text. The approach of simulating a sophisticated threat actor and creating a large, diverse dataset is commendable. While fine-tuning LLMs isn't entirely novel, the focus on realistic adversarial scenarios and the scale of the dataset contribute to the work's strength.

## Keywords

fine, ai, ai generated, fine tuned, generated, media, social, social media, text, tuned

## Links

- [Paper URL](https://arxiv.org/abs/2506.09975)

---
*Auto-generated on 2025-06-12 09:29:11 UTC*
