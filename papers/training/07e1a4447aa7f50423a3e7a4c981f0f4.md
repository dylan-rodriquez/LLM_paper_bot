# Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models

**Authors:** Roseline Polle, Agnes Norbury, Alexandra Livia Georgescu, Nicholas Cummins, Stefano Goria

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.23378v1 Announce Type: new 
Abstract: Speaker-dependent modelling can substantially improve performance in speech-based health monitoring applications. While mixed-effect models are commonly used for such speaker adaptation, they require computationally expensive retraining for each new observation, making them impractical in a production environment. We reformulate this task as a meta-learning problem and explore three approaches of increasing complexity: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Using pre-trained speech embeddings, we evaluate these methods on a large longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting time since sleep from speech as a function of fatigue, a symptom commonly associated with ill-health. Our results demonstrate that all meta-learning approaches tested outperformed both cross-sectional and conventional mixed-effects models, with a transformer-based method achieving the strongest performance.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This paper addresses a relevant problem – speaker-dependent voice fatigue modeling – and proposes a novel solution using meta-learning to overcome the limitations of traditional mixed-effects models. The evaluation on a large dataset is a strength, and the outperformance of existing methods suggests a solid methodology. The exploration of multiple meta-learning approaches (ensemble, prototypical networks, transformers) adds to the rigor and provides a comparative analysis.

## Keywords

models, based, approaches, learning, meta, meta learning, speaker, speech, commonly, dependent

## Links

- [Paper URL](https://arxiv.org/abs/2505.23378)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
