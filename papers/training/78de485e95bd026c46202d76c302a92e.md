# FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation

**Authors:** Srijith Nair, Michael Lin, Amirreza Talebi, Peizhong Ju, Elizabeth Bentley, Jia Liu

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 82.0/100

## Abstract

arXiv:2505.23182v1 Announce Type: new 
Abstract: Collaborative training methods like Federated Learning (FL) and Split Learning (SL) enable distributed machine learning without sharing raw data. However, FL assumes clients can train entire models, which is infeasible for large-scale models. In contrast, while SL alleviates the client memory constraint in FL by offloading most training to the server, it increases network latency due to its sequential nature. Other methods address the conundrum by using local loss functions for parallel client-side training to improve efficiency, but they lack server feedback and potentially suffer poor accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation Gradient Estimation), a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models. These auxiliary models periodically adapt to emulate server behavior on local datasets. We show that FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is the number of communication rounds. This result matches FedAvg, while significantly reducing communication costs and client memory requirements. Our empirical results also verify that it outperforms existing state-of-the-art FSL methods, offering both communication efficiency and accuracy.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 70.0/100  
**Sentiment Score:** 80.0/100

**Justification:** The paper addresses a relevant problem in federated learning â€“ the trade-off between client resource constraints and network latency in Split Learning. The proposed FSL-SAGE approach, using auxiliary models for gradient estimation, appears to be a novel way to mitigate these issues. While the abstract doesn't detail the mathematical rigor, the mention of a convergence rate suggests a solid theoretical foundation, and the problem is well-motivated. The potential for improved efficiency and accuracy in federated settings is significant.

## Keywords

learning, federated, fsl, models, server, split, split learning, client, communication, federated split

## Links

- [Paper URL](https://arxiv.org/abs/2505.23182)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
