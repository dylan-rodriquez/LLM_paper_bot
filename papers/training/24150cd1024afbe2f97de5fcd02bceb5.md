# Hey, That's My Data! Label-Only Dataset Inference in Large Language Models

**Authors:** Chen Xiong, Zihao Wang, Rui Zhu, Tsung-Yi Ho, Pin-Yu Chen, Jingwei Xiong, Haixu Tang, Lucila Ohno-Machado

**Published:** 2025-06-09 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 92.0/100

## Abstract

arXiv:2506.06057v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing by excelling at interpreting, reasoning about, and generating human language. However, their reliance on large-scale, often proprietary datasets poses a critical challenge: unauthorized usage of such data can lead to copyright infringement and significant financial harm. Existing dataset-inference methods typically depend on log probabilities to detect suspicious training material, yet many leading LLMs have begun withholding or obfuscating these signals. This reality underscores the pressing need for label-only approaches capable of identifying dataset membership without relying on internal model logits.
  We address this gap by introducing CatShift, a label-only dataset-inference framework that capitalizes on catastrophic forgetting: the tendency of an LLM to overwrite previously learned knowledge when exposed to new data. If a suspicious dataset was previously seen by the model, fine-tuning on a portion of it triggers a pronounced post-tuning shift in the model's outputs; conversely, truly novel data elicits more modest changes. By comparing the model's output shifts for a suspicious dataset against those for a known non-member validation set, we statistically determine whether the suspicious set is likely to have been part of the model's original training corpus. Extensive experiments on both open-source and API-based LLMs validate CatShift's effectiveness in logit-inaccessible settings, offering a robust and practical solution for safeguarding proprietary data.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 85.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This paper tackles a very important and timely problem â€“ dataset inference in LLMs, especially as log probabilities become less accessible. The proposed CatShift framework, leveraging catastrophic forgetting, appears to be a novel approach to label-only inference. The potential for detecting unauthorized data usage is significant, and the clear framing of the problem suggests a strong understanding of the current landscape. While the abstract doesn't detail the specifics of the method, the concept is promising and well-motivated.

## Keywords

dataset, data, model, language, suspicious, dataset inference, inference, label, large, llms

## Links

- [Paper URL](https://arxiv.org/abs/2506.06057)

---
*Auto-generated on 2025-06-09 09:30:26 UTC*
