# Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition

**Authors:** Hanting Chen (and Other Contributors), Yasheng Wang (and Other Contributors), Kai Han (and Other Contributors), Dong Li (and Other Contributors), Lin Li (and Other Contributors), Zhenni Bi (and Other Contributors), Jinpeng Li (and Other Contributors), Haoyu Wang (and Other Contributors), Fei Mi (and Other Contributors), Mingjian Zhu (and Other Contributors), Bin Wang (and Other Contributors), Kaikai Song (and Other Contributors), Yifei Fu (and Other Contributors), Xu He (and Other Contributors), Yu Luo (and Other Contributors), Chong Zhu (and Other Contributors), Quan He (and Other Contributors), Xueyu Wu (and Other Contributors), Wei He (and Other Contributors), Hailin Hu (and Other Contributors), Yehui Tang (and Other Contributors), Dacheng Tao (and Other Contributors), Xinghao Chen (and Other Contributors), Yunhe Wang (and Other Contributors)

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 36.5/100

## Abstract

arXiv:2505.22375v2 Announce Type: replace 
Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode for routine queries and a deeper "slow" mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners.

## Analysis

**Innovation Score:** 20.0/100
**Impact Score:** 16.0/100  
**Sentiment Score:** 57.5/100

**Justification:** Contains key LLM terms (bonus: 10)

## Keywords

embedded, pangu, pangu embedded, llm, mode, model, framework, latency, reasoning, stage

## Links

- [Paper URL](https://arxiv.org/abs/2505.22375)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
