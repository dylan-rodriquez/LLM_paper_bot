# Model-Preserving Adaptive Rounding

**Authors:** Albert Tseng, Zhaofeng Sun, Christopher De Sa

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.22988v1 Announce Type: new 
Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed model whose output distribution is as close to the original model's as possible. To do this tractably, almost all LLM PTQ algorithms quantize linear layers by independently minimizing the immediate activation error. However, this localized objective ignores the effect of subsequent layers, so reducing it does not necessarily give a closer model. In this work, we introduce Yet Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses Kronecker-factored approximations of each linear layer's Hessian with respect to the \textit{full model} KL divergence. YAQA consists of two components: Kronecker-factored sketches of the full layerwise Hessian that can be tractably computed for hundred-billion parameter LLMs, and a quantizer-independent rounding algorithm that uses these sketches and comes with theoretical guarantees. Across a wide range of models and quantizers, YAQA empirically reduces the KL divergence to the original model by $\approx 30\%$ while achieving state of the art performance on downstream tasks.

## Analysis

**Innovation Score:** 78.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 75.0/100

**Justification:** This paper addresses a crucial problem in LLM deployment – post-training quantization – and proposes a novel approach (YAQA) that moves beyond independent layer optimization. The use of Kronecker-factored Hessian approximations for the full model KL divergence is a clever technique to address tractability issues with large models. While the abstract doesn't detail experimental results, the theoretical guarantees and focus on a practical problem suggest strong potential. The 'Yet Another Quantization Algorithm' naming convention is slightly uninspired, but doesn't detract from the core idea.

## Keywords

model, algorithm, rounding, yaqa, adaptive, adaptive rounding, algorithm uses, divergence, factored, hessian

## Links

- [Paper URL](https://arxiv.org/abs/2505.22988)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
