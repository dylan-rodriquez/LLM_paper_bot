# Length-Controlled Margin-Based Preference Optimization without Reference Model

**Authors:** Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 35.1/100

## Abstract

arXiv:2502.14643v2 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at https://github.com/gengxuli/LMPO.

## Analysis

**Innovation Score:** 30.0/100
**Impact Score:** 8.0/100  
**Sentiment Score:** 51.6/100

**Justification:** High innovation indicators (score: 30); Technical sophistication (score: 35); Contains key LLM terms (bonus: 5)

## Keywords

length, lmpo, optimization, preference, based, margin, preference optimization, probability, controlled, controlled margin

## Links

- [Paper URL](https://arxiv.org/abs/2502.14643)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
