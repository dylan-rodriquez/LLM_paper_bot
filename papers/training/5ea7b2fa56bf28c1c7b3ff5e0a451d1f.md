# Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking

**Authors:** Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 39.6/100

## Abstract

arXiv:2502.12970v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performances across diverse domains. However, how safety of Large Language Models (LLMs) benefits from enhanced reasoning capabilities against jailbreak queries remains unexplored. To bridge this gap, in this paper, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates a safety-aware reasoning mechanism into LLMs' generation. This enables self-evaluation at each step of the reasoning process, forming safety pivot tokens as indicators of the safety status of responses. Furthermore, in order to improve the accuracy of predicting pivot tokens, we propose Contrastive Pivot Optimization (CPO), which enhances the model's perception of the safety status of given dialogues. LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their safety capabilities defending jailbreak attacks. Extensive experiments demonstrate that R2D effectively mitigates various attacks and improves overall safety, while maintaining the original performances. This highlights the substantial potential of safety-aware reasoning in improving robustness of LRMs and LLMs against various jailbreaks.

## Analysis

**Innovation Score:** 30.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 60.4/100

**Justification:** High innovation indicators (score: 30); Strong impact potential (score: 24); Positive sentiment analysis (score: 60.4); Contains key LLM terms (bonus: 10)

## Keywords

reasoning, safety, llms, aware, aware reasoning, defend, large, models, pivot, reasoning defend

## Links

- [Paper URL](https://arxiv.org/abs/2502.12970)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
