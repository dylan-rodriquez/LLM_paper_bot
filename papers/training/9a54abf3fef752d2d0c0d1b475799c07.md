# Highly Efficient and Effective LLMs with Multi-Boolean Architectures

**Authors:** Ba-Hien Tran, Van Minh Nguyen

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** stat.ML

**Significance Score:** 44.9/100

## Abstract

arXiv:2505.22811v1 Announce Type: cross 
Abstract: Weight binarization has emerged as a promising strategy to drastically reduce the complexity of large language models (LLMs). It is mainly classified into two approaches: post-training binarization and finetuning with training-aware binarization methods. The first approach, while having low complexity, leads to significant loss of information from the original LLMs, resulting in poor performance. The second approach, on the other hand, relies heavily on full-precision latent weights for gradient approximation of binary weights, which not only remains suboptimal but also introduces substantial complexity. In this paper, we introduce a novel framework that effectively transforms LLMs into multi-kernel Boolean parameters, for the first time, finetunes them directly in the Boolean domain, eliminating the need for expensive latent weights. This significantly reduces complexity during both finetuning and inference. Through extensive and insightful experiments across a wide range of LLMs, we demonstrate that our method outperforms recent ultra low-bit quantization and binarization methods.

## Analysis

**Innovation Score:** 40.0/100
**Impact Score:** 24.0/100  
**Sentiment Score:** 53.3/100

**Justification:** High innovation indicators (score: 40); Strong impact potential (score: 24); Contains key LLM terms (bonus: 10)

## Keywords

llms, binarization, complexity, boolean, weights, approach, binarization methods, finetuning, latent, latent weights

## Links

- [Paper URL](https://arxiv.org/abs/2505.22811)

---
*Auto-generated on 2025-05-30 11:01:12 UTC*
