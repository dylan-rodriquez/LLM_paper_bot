# ScienceMeter: Tracking Scientific Knowledge Updates in Language Models

**Authors:** Yike Wang, Shangbin Feng, Yulia Tsvetkov, Hannaneh Hajishirzi

**Published:** 2025-06-02 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 90.0/100

## Abstract

arXiv:2505.24302v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used to support scientific research, but their knowledge of scientific advancements can quickly become outdated. We introduce ScienceMeter, a new framework for evaluating scientific knowledge update methods over scientific knowledge spanning the past, present, and future. ScienceMeter defines three metrics: knowledge preservation, the extent to which models' understanding of previously learned papers are preserved; knowledge acquisition, how well scientific claims from newly introduced papers are acquired; and knowledge projection, the ability of the updated model to anticipate or generalize to related scientific claims that may emerge in the future. Using ScienceMeter, we examine the scientific knowledge of LLMs on claim judgment and generation tasks across a curated dataset of 15,444 scientific papers and 30,888 scientific claims from ten domains including medicine, biology, materials science, and computer science. We evaluate five representative knowledge update approaches including training- and inference-time methods. With extensive experiments, we find that the best-performing knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge. Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance. Cross-domain analysis reveals that performance on these objectives is correlated. Even when applying on specialized scientific LLMs, existing knowledge update methods fail to achieve these objectives collectively, underscoring that developing robust scientific knowledge update mechanisms is both crucial and challenging.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 80.0/100  
**Sentiment Score:** 92.0/100

**Justification:** This paper addresses a crucial problem in the age of LLMs â€“ their rapidly decaying scientific knowledge. The proposed ScienceMeter framework, with its three distinct metrics (preservation, acquisition, projection), offers a comprehensive and well-defined approach to evaluating knowledge update methods. The scale of the dataset (15,444 papers, 30,888 claims) suggests a rigorous evaluation, and the focus on ten domains adds breadth. The high sentiment score reflects the current research community's strong interest in addressing LLM knowledge limitations.

## Keywords

knowledge, scientific, knowledge update, methods, models, scientific knowledge, update, sciencemeter, claims, future

## Links

- [Paper URL](https://arxiv.org/abs/2505.24302)

---
*Auto-generated on 2025-06-02 09:29:50 UTC*
