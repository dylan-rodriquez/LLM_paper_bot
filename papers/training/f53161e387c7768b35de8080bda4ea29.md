# Directed Graph Grammars for Sequence-based Learning

**Authors:** Michael Sun, Orion Foo, Gang Liu, Wojciech Matusik, Jie Chen

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 82.0/100

## Abstract

arXiv:2505.22949v1 Announce Type: new 
Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in practice, with examples that include electronic circuits, Bayesian networks, and neural architectures. While many effective encoders exist for DAGs, it remains challenging to decode them in a principled manner, because the nodes of a DAG can have many different topological orders. In this work, we propose a grammar-based approach to constructing a principled, compact and equivalent sequential representation of a DAG. Specifically, we view a graph as derivations over an unambiguous grammar, where the DAG corresponds to a unique sequence of production rules. Equivalently, the procedure to construct such a description can be viewed as a lossless compression of the data. Such a representation has many uses, including building a generative model for graph generation, learning a latent space for property prediction, and leveraging the sequence representational continuity for Bayesian Optimization over structured data. Code is available at https://github.com/shiningsunnyday/induction.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 70.0/100  
**Sentiment Score:** 80.0/100

**Justification:** The paper addresses a challenging problem in graph representation learning â€“ the principled decoding of DAGs. The grammar-based approach to creating a sequential representation is a novel idea, potentially offering a compact and lossless compression method. The potential applications mentioned (generative models, latent spaces, representational continuity) suggest a broad range of downstream uses, though the abstract lacks specifics on experimental validation.

## Keywords

dag, graph, sequence, based, bayesian, dags, data, directed, grammar, graphs

## Links

- [Paper URL](https://arxiv.org/abs/2505.22949)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
