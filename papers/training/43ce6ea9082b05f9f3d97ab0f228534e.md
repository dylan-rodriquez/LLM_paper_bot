# Preference Learning with Response Time

**Authors:** Ayush Sawarni, Sahasrajit Sarmasarkar, Vasilis Syrgkanis

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 80.0/100

## Abstract

arXiv:2505.22820v1 Announce Type: new 
Abstract: This paper investigates the integration of response time data into human preference learning frameworks for more effective reward model elicitation. While binary preference data has become fundamental in fine-tuning foundation models, generative AI systems, and other large-scale models, the valuable temporal information inherent in user decision-making remains largely unexploited. We propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model, under which response time is informative of the preference strength. We develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning, matching the theoretical optimal rates that would be attained if the expected response times for each query were known a priori. Our theoretical analysis demonstrates that for linear reward functions, conventional preference learning suffers from error rates that scale exponentially with reward magnitude. In contrast, our response time-augmented approach reduces this to polynomial scaling, representing a significant improvement in sample efficiency. We extend these guarantees to non-parametric reward function spaces, establishing convergence properties for more complex, realistic reward models. Our extensive experiments validate our theoretical findings in the context of preference learning over images.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 82.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This paper addresses a relevant and increasingly important problem in preference learning â€“ leveraging all available data, including response times, to improve reward model elicitation. The use of the EZ model and development of Neyman-orthogonal loss functions with oracle convergence rates suggest a rigorous theoretical foundation. The potential to improve the efficiency and accuracy of preference learning, particularly for large models, is significant, and the topic aligns well with current research trends in AI alignment and human-in-the-loop learning.

## Keywords

preference, response, reward, learning, response time, time, preference learning, data, model, models

## Links

- [Paper URL](https://arxiv.org/abs/2505.22820)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
