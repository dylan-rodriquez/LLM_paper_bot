# Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study

**Authors:** Franck Gabriel, Fran\c{c}ois Ged, Maria Han Veiga, Emmanuel Schertzer

**Published:** 2025-05-31 | **Source:** arXiv RSS

**Categories:** cs.LG

**Significance Score:** 85.0/100

## Abstract

arXiv:2505.22841v1 Announce Type: new 
Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling, yet they can, in principle, be prone to memorization. In this case, their learned score overfits the finite dataset so that the reverse-time SDE samples are mostly training points. In this paper, we interpret the empirical score as a noisy version of the true score and show that its covariance matrix is asymptotically a re-weighted data PCA. In large dimension, the small time limit makes the noise variance blow up while simultaneously reducing spatial correlation. To reduce this variance, we introduce a kernel-smoothed empirical score and analyze its bias-variance trade-off. We derive asymptotic bounds on the Kullback-Leibler divergence between the true distribution and the one generated by the modified reverse SDE. Regularization on the score has the same effect as increasing the size of the training dataset, and thus helps prevent memorization. A spectral decomposition of the forward diffusion suggests better variance control under some regularity conditions of the true data distribution. Reverse diffusion with kernel-smoothed empirical score can be reformulated as a gradient descent drifted toward a Log-Exponential Double-Kernel Density Estimator (LED-KDE). This perspective highlights two regularization mechanisms taking place in denoising diffusions: an initial Gaussian kernel first diffuses mass isotropically in the ambient space, while a second kernel applied in score space concentrates and spreads that mass along the data manifold. Hence, even a straightforward regularization-without any learning-already mitigates memorization and enhances generalization. Numerically, we illustrate our results with several experiments on synthetic and MNIST datasets.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 78.0/100  
**Sentiment Score:** 80.0/100

**Justification:** This paper tackles a crucial issue in diffusion models – memorization – and offers a theoretically grounded approach to mitigate it. The analysis of the empirical score as a noisy version of the true score, and its connection to data PCA, is insightful. The kernel smoothing technique and the derived asymptotic bounds suggest a rigorous methodology, though the 'spectral deco...' truncation in the abstract leaves some ambiguity about the full scope of the work.

## Keywords

score, kernel, variance, diffusion, data, empirical, empirical score, kernel smoothed, memorization, regularization

## Links

- [Paper URL](https://arxiv.org/abs/2505.22841)

---
*Auto-generated on 2025-05-31 09:25:12 UTC*
