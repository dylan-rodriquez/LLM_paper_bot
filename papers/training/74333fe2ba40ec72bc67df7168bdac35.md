# FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian

**Authors:** Sara Papi, Marco Gaido, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri

**Published:** 2025-05-30 | **Source:** arXiv RSS

**Categories:** cs.CL

**Significance Score:** 80.0/100

## Abstract

arXiv:2505.22759v1 Announce Type: new 
Abstract: The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, their closed nature--with inaccessible training data and code--poses major reproducibility and fair evaluation challenges. While other domains have made substantial progress toward open science by developing fully transparent models trained on open-source (OS) code and data, similar efforts in speech remain limited. To fill this gap, we introduce FAMA, the first family of open science SFMs for English and Italian, trained on 150k+ hours of OS speech data. Moreover, we present a new dataset containing 16k hours of cleaned and pseudo-labeled speech for both languages. Results show that FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster. All artifacts, including code, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research.

## Analysis

**Innovation Score:** 75.0/100
**Impact Score:** 85.0/100  
**Sentiment Score:** 90.0/100

**Justification:** This paper addresses a crucial issue in speech processing â€“ the lack of open-source, reproducible foundation models. The creation of FAMA, trained on a substantial open-source dataset and released with all artifacts, is a significant step towards democratizing access to this technology. The reported speed improvements further enhance its practical value, and the inclusion of Italian alongside English broadens its applicability.

## Keywords

speech, open, code, data, fama, models, open science, os, science, sfms

## Links

- [Paper URL](https://arxiv.org/abs/2505.22759)

---
*Auto-generated on 2025-05-30 13:44:12 UTC*
