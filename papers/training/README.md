# Training Papers

This directory contains papers related to training in large language models and AI.

## Papers (55 total)

### Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text

**Score:** 92.0 | **Published:** 2025-06-10 | **Authors:** Yize Cheng, Vinu Sankar Sadasivan, Mehrdad Saberi, Shoumik Saha, Soheil Feizi

arXiv:2506.07001v1 Announce Type: new 
Abstract: The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. ...

[📄 Full Paper](https://arxiv.org/abs/2506.07001) | [📝 Analysis](a6573a31e2e326492222949a6eb0e962.md)

---

### Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning

**Score:** 90.0 | **Published:** 2025-06-10 | **Authors:** Jiaxing Guo, Wenjie Yang, Shengzhong Zhang, Tongshan Xu, Lun Du, Da Zheng, Zengfeng Huang

arXiv:2506.06877v1 Announce Type: new 
Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable success in mathematical problem-solving. However, this success often masks a...

[📄 Full Paper](https://arxiv.org/abs/2506.06877) | [📝 Analysis](f16596aa9b2c4ef614d8536ca4c25936.md)

---

### RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality

**Score:** 90.0 | **Published:** 2025-06-10 | **Authors:** Chenlong Zhang, Zhuoran Jin, Hongbang Yuan, Jiaheng Wei, Tong Zhou, Kang Liu, Jun Zhao, Yubo Chen

arXiv:2506.07171v1 Announce Type: new 
Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive, uncurated corpora has raised growing concerns about the inclusion of sens...

[📄 Full Paper](https://arxiv.org/abs/2506.07171) | [📝 Analysis](3b6e2f40bed414b9d51d7ff936c23c77.md)

---

### FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian

**Score:** 80.0 | **Published:** 2025-05-30 | **Authors:** Sara Papi, Marco Gaido, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri

arXiv:2505.22759v1 Announce Type: new 
Abstract: The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, the...

[📄 Full Paper](https://arxiv.org/abs/2505.22759) | [📝 Analysis](74333fe2ba40ec72bc67df7168bdac35.md)

---

### Pre-Training Curriculum for Multi-Token Prediction in Language Models

**Score:** 75.0 | **Published:** 2025-05-30 | **Authors:** Ansar Aynetdinov, Alan Akbik

arXiv:2505.22757v1 Announce Type: new 
Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective for language models. Rather than predicting only the next token (NTP), MTP p...

[📄 Full Paper](https://arxiv.org/abs/2505.22757) | [📝 Analysis](12ab24bf09f2216240eb4013cc457122.md)

---

### LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning

**Score:** 52.1 | **Published:** 2025-05-30 | **Authors:** Tengfei Lyu, Siyuan Feng, Hao Liu, Hai Yang

arXiv:2505.22695v1 Announce Type: new 
Abstract: Ride-hailing platforms face significant challenges in optimizing order dispatching and driver repositioning operations in dynamic urban environments. T...

[📄 Full Paper](https://arxiv.org/abs/2505.22695) | [📝 Analysis](0c431b9a76e90eabf1a8e1152bbaab40.md)

---

### DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning

**Score:** 51.2 | **Published:** 2025-05-30 | **Authors:** Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu

arXiv:2505.23754v1 Announce Type: new 
Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated th...

[📄 Full Paper](https://arxiv.org/abs/2505.23754) | [📝 Analysis](9640096705f6ab1a7b05e6acebada1c5.md)

---

### cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning

**Score:** 50.0 | **Published:** 2025-05-30 | **Authors:** Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich

arXiv:2505.22914v1 Announce Type: cross 
Abstract: Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using...

[📄 Full Paper](https://arxiv.org/abs/2505.22914) | [📝 Analysis](04693cd75574f88755c8fcc69d7444be.md)

---

### LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection

**Score:** 47.8 | **Published:** 2025-05-30 | **Authors:** Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou

arXiv:2505.03793v2 Announce Type: replace-cross 
Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the...

[📄 Full Paper](https://arxiv.org/abs/2505.03793) | [📝 Analysis](b73448c562fcd30047a7d631e478d8a0.md)

---

### Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling

**Score:** 45.7 | **Published:** 2025-05-30 | **Authors:** Yanzhao Hou, Jiaxiang Geng, Boyu Li, Xiaofeng Tao, Juncheng Wang, Xiaodong Xu, Bing Luo

arXiv:2505.23555v1 Announce Type: new 
Abstract: Federated LoRA has emerged as a promising technique for efficiently fine-tuning large language models (LLMs) on distributed devices by reducing the num...

[📄 Full Paper](https://arxiv.org/abs/2505.23555) | [📝 Analysis](4ce6bbb3f3ab9976aa8b01c5ead9d707.md)

---

### Highly Efficient and Effective LLMs with Multi-Boolean Architectures

**Score:** 44.9 | **Published:** 2025-05-30 | **Authors:** Ba-Hien Tran, Van Minh Nguyen

arXiv:2505.22811v1 Announce Type: cross 
Abstract: Weight binarization has emerged as a promising strategy to drastically reduce the complexity of large language models (LLMs). It is mainly classified...

[📄 Full Paper](https://arxiv.org/abs/2505.22811) | [📝 Analysis](9a54abf3fef752d2d0c0d1b475799c07.md)

---

### Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking

**Score:** 44.8 | **Published:** 2025-05-30 | **Authors:** Athanasios Glentis, Jiaxiang Li, Qiulin Shang, Andi Han, Ioannis Tsaknakis, Quan Wei, Mingyi Hong

arXiv:2505.22922v1 Announce Type: cross 
Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple domains, large language models (LLMs) have grown at an unprecedented rate,...

[📄 Full Paper](https://arxiv.org/abs/2505.22922) | [📝 Analysis](7e67a163277f46c9e3da1c0e53d5b62c.md)

---

### EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models

**Score:** 44.6 | **Published:** 2025-05-30 | **Authors:** Yuzhen Xiao, Jiahe Song, Yongxin Xu, Ruizhe Zhang, Yiqi Xiao, Xin Lu, Runchuan Zhu, Bowen Jiang, Junfeng Zhao

arXiv:2505.23038v1 Announce Type: new 
Abstract: In-Context Learning (ICL) technique based on Large Language Models (LLMs) has gained prominence in Named Entity Recognition (NER) tasks for its lower c...

[📄 Full Paper](https://arxiv.org/abs/2505.23038) | [📝 Analysis](4944b7697582de3e7dd2f4ed75f2cb92.md)

---

### Accelerating RLHF Training with Reward Variance Increase

**Score:** 44.4 | **Published:** 2025-05-30 | **Authors:** Zonglin Yang, Zhexuan Gu, Houduo Qi, Yancheng Yuan

arXiv:2505.23247v1 Announce Type: cross 
Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human val...

[📄 Full Paper](https://arxiv.org/abs/2505.23247) | [📝 Analysis](a8af982fe6c32c4a656bb98c1f7df325.md)

---

### BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model

**Score:** 44.2 | **Published:** 2025-05-30 | **Authors:** Adibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan Duan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, Bo Wang

arXiv:2505.23579v1 Announce Type: new 
Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA founda...

[📄 Full Paper](https://arxiv.org/abs/2505.23579) | [📝 Analysis](5cc95452868e1558a4031987813d1178.md)

---

### DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification

**Score:** 43.9 | **Published:** 2025-05-30 | **Authors:** Youssef Mohamed, Noran Mohamed, Khaled Abouhashad, Feilong Tang, Sara Atito, Shoaib Jameel, Imran Razzak, Ahmed B. Zaky

arXiv:2505.23595v1 Announce Type: cross 
Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains such as medical imaging by enabling shared representation learning, eff...

[📄 Full Paper](https://arxiv.org/abs/2505.23595) | [📝 Analysis](36f2faf80d6f650bdd73a39143c64ecf.md)

---

### Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time

**Score:** 43.7 | **Published:** 2025-05-30 | **Authors:** Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi

arXiv:2505.23729v1 Announce Type: new 
Abstract: Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches t...

[📄 Full Paper](https://arxiv.org/abs/2505.23729) | [📝 Analysis](9f969e328ab973a5b3dab1aa61c429b3.md)

---

### Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek

**Score:** 42.6 | **Published:** 2025-05-30 | **Authors:** Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou

arXiv:2505.17702v2 Announce Type: replace-cross 
Abstract: The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent rese...

[📄 Full Paper](https://arxiv.org/abs/2505.17702) | [📝 Analysis](ba288a17506b554dc57acf2f95c57bff.md)

---

### GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation

**Score:** 41.9 | **Published:** 2025-05-30 | **Authors:** Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro

arXiv:2410.08475v3 Announce Type: replace-cross 
Abstract: Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (L...

[📄 Full Paper](https://arxiv.org/abs/2410.08475) | [📝 Analysis](25672b819e63d4f28b1432b09cfd4689.md)

---

### MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning

**Score:** 41.7 | **Published:** 2025-05-30 | **Authors:** Dacao Zhang, Kun Zhang, Shimao Chu, Le Wu, Xin Li, Si Wei

arXiv:2505.22694v1 Announce Type: new 
Abstract: With the rapid development of Large Language Models (LLMs), Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant attention, which aim...

[📄 Full Paper](https://arxiv.org/abs/2505.22694) | [📝 Analysis](832efeaf4de9661c058125dd0a3ffd22.md)

---

### BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models

**Score:** 41.0 | **Published:** 2025-05-30 | **Authors:** Yupeng Chang, Yi Chang, Yuan Wu

arXiv:2408.04556v5 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLM...

[📄 Full Paper](https://arxiv.org/abs/2408.04556) | [📝 Analysis](5ac5419a0a2df642aeb3e08f4192b853.md)

---

### ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering

**Score:** 40.1 | **Published:** 2025-05-30 | **Authors:** Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen

arXiv:2505.23723v1 Announce Type: new 
Abstract: The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. Ho...

[📄 Full Paper](https://arxiv.org/abs/2505.23723) | [📝 Analysis](5c135a56f59b75e821427d17b16b3d30.md)

---

### ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind

**Score:** 39.6 | **Published:** 2025-05-30 | **Authors:** Peixuan Han, Zijia Liu, Jiaxuan You

arXiv:2505.22961v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably...

[📄 Full Paper](https://arxiv.org/abs/2505.22961) | [📝 Analysis](82072d540788df88d130d2cd9f01e8b7.md)

---

### MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration

**Score:** 39.6 | **Published:** 2025-05-30 | **Authors:** Zhitao He (May), Sandeep Polisetty (May), Zhiyuan Fan (May), Yuchen Huang (May), Shujin Wu (May), Yi R. (May),  Fung

arXiv:2505.23224v1 Announce Type: new 
Abstract: In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoni...

[📄 Full Paper](https://arxiv.org/abs/2505.23224) | [📝 Analysis](a309c7acc03c2b75328d55531ef0452b.md)

---

### Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking

**Score:** 39.6 | **Published:** 2025-05-30 | **Authors:** Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha

arXiv:2502.12970v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performances across diverse domains. However, how safety of Large Language Models (LLMs)...

[📄 Full Paper](https://arxiv.org/abs/2502.12970) | [📝 Analysis](5ea7b2fa56bf28c1c7b3ff5e0a451d1f.md)

---

### EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions

**Score:** 39.5 | **Published:** 2025-05-30 | **Authors:** Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xiaolu Zhang, Jun Zhou, Yuxiang Peng, Li Zheng, Chong Teng, Donghong Ji, Zhuang Li

arXiv:2505.23473v1 Announce Type: new 
Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary ...

[📄 Full Paper](https://arxiv.org/abs/2505.23473) | [📝 Analysis](80350614b95cbbda15dea8143fceaab8.md)

---

### LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization

**Score:** 39.5 | **Published:** 2025-05-30 | **Authors:** Yupeng Chang, Chenlu Guo, Yi Chang, Yuan Wu

arXiv:2502.14538v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), enable efficient adaptation of large language models (LLMs) via...

[📄 Full Paper](https://arxiv.org/abs/2502.14538) | [📝 Analysis](30d7828d6b19fc3ebcd09b896ecdc8df.md)

---

### Self-Critique and Refinement for Faithful Natural Language Explanations

**Score:** 39.4 | **Published:** 2025-05-30 | **Authors:** Yingming Wang, Pepa Atanasova

arXiv:2505.22823v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), natural language explanations (NLEs) have become increasingly important for understanding m...

[📄 Full Paper](https://arxiv.org/abs/2505.22823) | [📝 Analysis](f6b823641d1991933f96caeb07194ba0.md)

---

### Diversity-Aware Policy Optimization for Large Language Model Reasoning

**Score:** 38.7 | **Published:** 2025-05-30 | **Authors:** Jian Yao, Ran Cheng, Xingyu Wu, Jibin Wu, Kay Chen Tan

arXiv:2505.23433v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek R1, which has inspired...

[📄 Full Paper](https://arxiv.org/abs/2505.23433) | [📝 Analysis](0ac76938f849552863ae80d4f2e3eabc.md)

---

### Daunce: Data Attribution through Uncertainty Estimation

**Score:** 38.6 | **Published:** 2025-05-30 | **Authors:** Xingyuan Pan, Chenlu Ye, Joseph Melkonian, Jiaqi W. Ma, Tong Zhang

arXiv:2505.23223v1 Announce Type: new 
Abstract: Training data attribution (TDA) methods aim to identify which training examples influence a model's predictions on specific test data most. By quantify...

[📄 Full Paper](https://arxiv.org/abs/2505.23223) | [📝 Analysis](89a8825dc084a28a41331cf62477f55a.md)

---

### Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms

**Score:** 38.6 | **Published:** 2025-05-30 | **Authors:** Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra

arXiv:2501.13977v3 Announce Type: replace 
Abstract: Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement...

[📄 Full Paper](https://arxiv.org/abs/2501.13977) | [📝 Analysis](a535681ba5f4ef65e8912046dd0d4d21.md)

---

### Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization

**Score:** 38.5 | **Published:** 2025-05-30 | **Authors:** Mingzhe Du, Luu Tuan Tuan, Yue Liu, Yuhao Qing, Dong Huang, Xinyi He, Qian Liu, Zejun Ma, See-kiong Ng

arXiv:2505.23387v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world de...

[📄 Full Paper](https://arxiv.org/abs/2505.23387) | [📝 Analysis](d716032d8dfabd73cc639366419369d7.md)

---

### GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning

**Score:** 38.3 | **Published:** 2025-05-30 | **Authors:** Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang

arXiv:2502.12913v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face ...

[📄 Full Paper](https://arxiv.org/abs/2502.12913) | [📝 Analysis](5d21d180d1b1f09fa6c34131fe58d951.md)

---

### Learning to Search for Vehicle Routing with Multiple Time Windows

**Score:** 38.3 | **Published:** 2025-05-30 | **Authors:** Kuan Xu, Zhiguang Cao, Chenlong Zheng, Linong Liu

arXiv:2505.23098v1 Announce Type: new 
Abstract: In this study, we propose a reinforcement learning-based adaptive variable neighborhood search (RL-AVNS) method designed for effectively solving the Ve...

[📄 Full Paper](https://arxiv.org/abs/2505.23098) | [📝 Analysis](152e928403042d375075fe2e0dbede15.md)

---

### Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage

**Score:** 37.9 | **Published:** 2025-05-30 | **Authors:** Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang

arXiv:2505.08167v3 Announce Type: replace 
Abstract: The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLM...

[📄 Full Paper](https://arxiv.org/abs/2505.08167) | [📝 Analysis](32c66f9306233a120a40994fdd1eea1c.md)

---

### One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory

**Score:** 37.8 | **Published:** 2025-05-30 | **Authors:** Chenhao Zheng, Jieyu Zhang, Mohammadreza Salehi, Ziqi Gao, Vishnu Iyengar, Norimasa Kobori, Quan Kong, Ranjay Krishna

arXiv:2505.23617v1 Announce Type: cross 
Abstract: Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches,...

[📄 Full Paper](https://arxiv.org/abs/2505.23617) | [📝 Analysis](1ce4060fdd455540f63666f099f64072.md)

---

### Table-R1: Inference-Time Scaling for Table Reasoning

**Score:** 37.6 | **Published:** 2025-05-30 | **Authors:** Zheyuan Yang, Lyuhao Chen, Arman Cohan, Yilun Zhao

arXiv:2505.23621v1 Announce Type: new 
Abstract: In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategi...

[📄 Full Paper](https://arxiv.org/abs/2505.23621) | [📝 Analysis](d58df1a630c4f61d2b0d5b1501107184.md)

---

### MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment

**Score:** 37.5 | **Published:** 2025-05-30 | **Authors:** John Halloran

arXiv:2505.23634v1 Announce Type: new 
Abstract: The model context protocol (MCP) has been widely adapted as an open standard enabling the seamless integration of generative AI agents. However, recent...

[📄 Full Paper](https://arxiv.org/abs/2505.23634) | [📝 Analysis](c061cd2d2d923ce97f5a1eaf64064a51.md)

---

### LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline

**Score:** 37.4 | **Published:** 2025-05-30 | **Authors:** Biao Fu, Minpeng Liao, Kai Fan, Chengxi Li, Liang Zhang, Yidong Chen, Xiaodong Shi

arXiv:2504.09570v2 Announce Type: replace 
Abstract: When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple p...

[📄 Full Paper](https://arxiv.org/abs/2504.09570) | [📝 Analysis](4afb85a0fc2b1142b810a3cc30b02f1b.md)

---

### HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model

**Score:** 37.1 | **Published:** 2025-05-30 | **Authors:** Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu

arXiv:2503.12941v2 Announce Type: replace 
Abstract: Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets,...

[📄 Full Paper](https://arxiv.org/abs/2503.12941) | [📝 Analysis](47c2ce6cf6cfea1656e63dedb740e05b.md)

---

### Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs

**Score:** 36.6 | **Published:** 2025-05-30 | **Authors:** Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang

arXiv:2410.03553v3 Announce Type: replace 
Abstract: Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate predi...

[📄 Full Paper](https://arxiv.org/abs/2410.03553) | [📝 Analysis](3bf215c47f4a2b57da02ccf6bfebc2e1.md)

---

### Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition

**Score:** 36.5 | **Published:** 2025-05-30 | **Authors:** Hanting Chen (and Other Contributors), Yasheng Wang (and Other Contributors), Kai Han (and Other Contributors), Dong Li (and Other Contributors), Lin Li (and Other Contributors), Zhenni Bi (and Other Contributors), Jinpeng Li (and Other Contributors), Haoyu Wang (and Other Contributors), Fei Mi (and Other Contributors), Mingjian Zhu (and Other Contributors), Bin Wang (and Other Contributors), Kaikai Song (and Other Contributors), Yifei Fu (and Other Contributors), Xu He (and Other Contributors), Yu Luo (and Other Contributors), Chong Zhu (and Other Contributors), Quan He (and Other Contributors), Xueyu Wu (and Other Contributors), Wei He (and Other Contributors), Hailin Hu (and Other Contributors), Yehui Tang (and Other Contributors), Dacheng Tao (and Other Contributors), Xinghao Chen (and Other Contributors), Yunhe Wang (and Other Contributors)

arXiv:2505.22375v2 Announce Type: replace 
Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring f...

[📄 Full Paper](https://arxiv.org/abs/2505.22375) | [📝 Analysis](b254c55ad321f5ae57cc8cc7a58ac851.md)

---

### GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation

**Score:** 36.4 | **Published:** 2025-05-30 | **Authors:** Tao Feng, Yihang Sun, Jiaxuan You

arXiv:2503.12600v2 Announce Type: replace 
Abstract: The powerful capabilities of Large Language Models (LLMs) have led to their growing use in evaluating human-generated content, particularly in eval...

[📄 Full Paper](https://arxiv.org/abs/2503.12600) | [📝 Analysis](4e4156bee9b074c148fd55769165f374.md)

---

### STeCa: Step-level Trajectory Calibration for LLM Agent Learning

**Score:** 36.4 | **Published:** 2025-05-30 | **Authors:** Hanlin Wang, Jian Wang, Chak Tou Leong, Wenjie Li

arXiv:2502.14276v2 Announce Type: replace-cross 
Abstract: Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existin...

[📄 Full Paper](https://arxiv.org/abs/2502.14276) | [📝 Analysis](f9ced87490a0bdc3245ff0e1c3337a65.md)

---

### Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters

**Score:** 36.3 | **Published:** 2025-05-30 | **Authors:** Hayden Moore, Sirui Qi, Ninad Hogade, Dejan Milojicic, Cullen Bash, Sudeep Pasricha

arXiv:2505.23554v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs con...

[📄 Full Paper](https://arxiv.org/abs/2505.23554) | [📝 Analysis](b12abb24ec5b4b20376880a9f6e654ae.md)

---

### Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization

**Score:** 36.2 | **Published:** 2025-05-30 | **Authors:** Shohei Enomoto

arXiv:2505.23173v1 Announce Type: new 
Abstract: Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in rea...

[📄 Full Paper](https://arxiv.org/abs/2505.23173) | [📝 Analysis](07765244da5f2c68327780096280ac17.md)

---

### Training Language Models to Generate Quality Code with Program Analysis Feedback

**Score:** 36.2 | **Published:** 2025-05-30 | **Authors:** Feng Yao, Zilong Wang, Liyuan Liu, Junxia Cui, Li Zhong, Xiaohan Fu, Haohui Mai, Vish Krishnan, Jianfeng Gao, Jingbo Shang

arXiv:2505.22704v1 Announce Type: new 
Abstract: Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, pa...

[📄 Full Paper](https://arxiv.org/abs/2505.22704) | [📝 Analysis](d60e8e83eccd27feff7f306dc9043fcc.md)

---

### Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification

**Score:** 36.2 | **Published:** 2025-05-30 | **Authors:** Denis Mamba Kabala, Adel Hafiane, Laurent Bobelin, Raphael Canals

arXiv:2505.23063v1 Announce Type: new 
Abstract: Crop disease detection and classification is a critical challenge in agriculture, with major implications for productivity, food security, and environm...

[📄 Full Paper](https://arxiv.org/abs/2505.23063) | [📝 Analysis](f844f00481555099df44cfd7c993422e.md)

---

### Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO

**Score:** 36.0 | **Published:** 2025-05-30 | **Authors:** Kaiyang Guo, Yinchuan Li, Zhitang Chen

arXiv:2505.23316v1 Announce Type: new 
Abstract: Direct alignment methods typically optimize large language models (LLMs) by contrasting the likelihoods of preferred versus dispreferred responses. Whi...

[📄 Full Paper](https://arxiv.org/abs/2505.23316) | [📝 Analysis](a9ef727383e3a41c6198a924460f9647.md)

---

### Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models

**Score:** 36.0 | **Published:** 2025-05-30 | **Authors:** Jinwen Chen, Hainan Zhang, Fei Sun, Qinnan Zhang, Sijia Wen, Ziwei Wang, Zhiming Zheng

arXiv:2505.23015v1 Announce Type: new 
Abstract: Fine-tuning LLMs with datasets containing stealthy backdoors from publishers poses security risks to downstream applications. Mainstream detection meth...

[📄 Full Paper](https://arxiv.org/abs/2505.23015) | [📝 Analysis](88cf260905237993fa309d806bf125c4.md)

---

### BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection

**Score:** 35.8 | **Published:** 2025-05-30 | **Authors:** Shams Tarek, Dipayan Saha, Sujan Kumar Saha, Farimah Farahmandi

arXiv:2505.22878v1 Announce Type: cross 
Abstract: The current landscape of system-on-chips (SoCs) security verification faces challenges due to manual, labor-intensive, and inflexible methodologies. ...

[📄 Full Paper](https://arxiv.org/abs/2505.22878) | [📝 Analysis](c5955174c7bc0641c184cd2dde999587.md)

---

### Understanding Bias Reinforcement in LLM Agents Debate

**Score:** 35.5 | **Published:** 2025-05-30 | **Authors:** Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun

arXiv:2503.16814v2 Announce Type: replace-cross 
Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensu...

[📄 Full Paper](https://arxiv.org/abs/2503.16814) | [📝 Analysis](dbebedc999bbc077bc30a349365b87a3.md)

---

### AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models

**Score:** 35.4 | **Published:** 2025-05-30 | **Authors:** Jinchuan Zhang, Lu Yin, Yan Zhou, Songlin Hu

arXiv:2505.23020v1 Announce Type: cross 
Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge providers" to "action executors", a trend that while expanding LLMs' cap...

[📄 Full Paper](https://arxiv.org/abs/2505.23020) | [📝 Analysis](a5cc25f751a58324e60d01470506fb5e.md)

---

### Length-Controlled Margin-Based Preference Optimization without Reference Model

**Score:** 35.1 | **Published:** 2025-05-30 | **Authors:** Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu

arXiv:2502.14643v2 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), ...

[📄 Full Paper](https://arxiv.org/abs/2502.14643) | [📝 Analysis](fcb19b58e28079feb1e2ec2d1a3dc409.md)

---

### PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow

**Score:** 35.0 | **Published:** 2025-05-30 | **Authors:** Michael Klamkin, Mathieu Tanneau, Pascal Van Hentenryck

arXiv:2505.22825v1 Announce Type: cross 
Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have recently garnered significant attention, reflecting a broader trend of le...

[📄 Full Paper](https://arxiv.org/abs/2505.22825) | [📝 Analysis](5cd4de52b77b2810e4b93b77ed4fef53.md)

---


*Last updated: 2025-06-10 09:29:56 UTC*
