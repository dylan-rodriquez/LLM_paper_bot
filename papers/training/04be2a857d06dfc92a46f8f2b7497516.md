# A Survey on Large Language Models for Mathematical Reasoning

**Authors:** Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, Yang Yu

**Published:** 2025-06-11 | **Source:** arXiv RSS

**Categories:** cs.AI

**Significance Score:** 92.0/100

## Abstract

arXiv:2506.08446v1 Announce Type: cross 
Abstract: Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and "test-time scaling". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains.

## Analysis

**Innovation Score:** 65.0/100
**Impact Score:** 85.0/100  
**Sentiment Score:** 88.0/100

**Justification:** This survey paper addresses a highly significant and timely topic â€“ the application of LLMs to mathematical reasoning. The categorization into comprehension and answer generation phases provides a useful framework for understanding the field's progress. While the abstract doesn't detail novel *methods*, a comprehensive survey of existing techniques (prompting, fine-tuning, CoT) is valuable, and the mention of 'test-time scaling' suggests awareness of current research directions. The identified challenges (capacity, efficiency, generalization) are crucial for future work.

## Keywords

reasoning, mathematical, mathematical reasoning, llms, models, survey, cot, enhancing, fine, fine tuning

## Links

- [Paper URL](https://arxiv.org/abs/2506.08446)

---
*Auto-generated on 2025-06-11 09:29:34 UTC*
